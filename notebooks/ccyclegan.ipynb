{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ccyclegan.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1auNkyV1smaOcPqNmjHH4VimSQFzugP2C","authorship_tag":"ABX9TyMYU9J8CYA5kC7tZ90qyqgf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <font color='red'>**Data loader**</font>\n","## **Useful libraries**"],"metadata":{"id":"xlI0dmYeXotw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAZvBb8PaiBk"},"outputs":[],"source":["import scipy\n","from glob import glob\n","import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import cv2"]},{"cell_type":"code","source":["#TALE SECOND PART:\n","class DataLoader():\n","  \"\"\" Data loader method: loader initialization on training or test batch\n","\n","  Parameters\n","  -------------------   \n","  dataset_name: string- dataset name\n","  img_res: array- image shape (n_rows, n_cols,n_channels)\n","  path_csv: string- path csv (que contiene el csv?, es importante?***)\n","  use_test_in_batch: boolean- decides if the data is test or not\n","  normalize: boolean- for image normalization\n","  \"\"\"\n","  #csv dataset has three columns: Emotion: (label from 0 to 6)\n","                              #   Pixels: (pixel values from 48X48 image)\n","                              #   Usage: split to be used (train or test) \n","  def __init__(self, dataset_name, img_res=(48, 48,1), path_csv=None, use_test_in_batch=False, normalize=True):\n","\n","    self.dataset_name = dataset_name\n","    self.img_res = img_res\n","    # images and labels vectors for train and test \n","    self.img_vect_train = None \n","    self.img_vect_test = None \n","    self.lab_vect_train = None \n","    self.lab_vect_test = None \n","    self.path_csv = path_csv \n","    ## labels dict\n","    self.lab_dict = {0: \"Angry\", 1: \"Disgust\" , 2: \"Fear\" , 3: \"Happy\" , 4: \"Sad\" , \n","                    5: \"Surprise\" , 6: \"Neutral\"}\n","    self.use_test_in_batch = use_test_in_batch\n","    self.normalize = normalize \n","    ## load dataset \n","    self._load_internally()\n","\n","  \n","  def _load_internally(self):  \n","    \"\"\"\n","\n","    \"\"\"\n","\n","    print(\">> loading \"+str(self.dataset_name)+\" ...\") \n","    #reading csv dataset\n","    if self.dataset_name == 'fer2013': #change dataset***\n","      if self.path_csv is None:\n","        raw_data = pd.read_csv('/content/drive/MyDrive/Maestria/Polyps/experiments/conditional CycleGan/fer2013.csv')\n","      else:\n","        raw_data = pd.read_csv(self.path_csv)\n","    else:\n","      raise Exception(\"dataset not supported:\"+str(self.dataset_name))\n","    \n","    #reading train and test split \n","    n_train = np.sum(raw_data['Usage'] == 'Training')\n","    n_test = np.sum(raw_data['Usage'] != 'Training')\n","    assert n_train + n_test == len(raw_data)\n","    \n","    #\"batch\" of training and test data (#train/test samples, img_w, img_h, img_ch, dataType)\n","    self.img_vect_train = np.zeros( (n_train, self.img_res[0],\n","                                      self.img_res[1], self.img_res[2]) , 'float32')\n","    self.img_vect_test = np.zeros( (n_test, self.img_res[0],\n","                                      self.img_res[1], self.img_res[2]) , 'float32')\n","    self.lab_vect_train = np.zeros(n_train, 'int32')\n","    self.lab_vect_test = np.zeros(n_test, 'int32')\n","    \n","    i_train , i_test = 0,0\n","    #pass throught all data\n","    print(\"passing throught all data...\")\n","    for i in range(len(raw_data)):  \n","      #get pixels for i data\n","      img = raw_data[\"pixels\"][i] \n","      x_pixels = np.array(img.split(\" \"), 'float32')\n","      #normalize\n","      if self.normalize:\n","        x_pixels = x_pixels/127.5 - 1.\n","      #reshape into image matrix\n","      x_pixels = x_pixels.reshape(self.img_res)\n","      #get set (train or test)\n","      us = raw_data[\"Usage\"][i]\n","      #save into image vect set for training or test\n","      if us == 'Training':\n","        self.img_vect_train[i_train] = x_pixels\n","        self.lab_vect_train[i_train] = int(raw_data[\"emotion\"][i]) \n","        i_train = i_train + 1\n","      else:\n","        self.img_vect_test[i_test] = x_pixels\n","        self.lab_vect_test[i_test] = int(raw_data[\"emotion\"][i]) \n","        i_test = i_test + 1\n","    \n","    #for check \n","    assert i_train == len(self.img_vect_train) \n","    assert i_train == len(self.lab_vect_train) \n","    assert i_test == len(self.lab_vect_test) \n","    assert i_test == len(self.img_vect_test) \n","    \n","    print(\"> loaded train:\",len(self.img_vect_train),\"   - test:\",len(self.lab_vect_test) )\n","    \n","    #make RGB test/train images \"batch\" (#samples, width, high, 3)\n","    print(\"converting img to RGB...\")\n","    self.img_vect_test_RGB = np.zeros((self.img_vect_test.shape[0], self.img_res[0],\n","                                        self.img_res[1], 3))\n","    #go through test RGB and convert gray-->RGB\n","    for i in range(self.img_vect_test_RGB.shape[0]):\n","      self.img_vect_test_RGB[i] = cv2.cvtColor(self.img_vect_test[i], cv2.COLOR_GRAY2RGB)\n","    \n","    print(\"ready test RGB!\")\n","        \n","    self.img_vect_train_RGB = np.zeros((self.img_vect_train.shape[0],self.img_res[0],self.img_res[1],3))\n","    for i in range(self.img_vect_train_RGB.shape[0]):\n","      self.img_vect_train_RGB[i] = cv2.cvtColor(self.img_vect_train[i], cv2.COLOR_GRAY2RGB)\n","    print(\"ready train RGB!\")\n","\n","    ## its seems to we can read the images at the same time (changed***)\n","    #leo = cv2.imread('./images/leo_gray__crop_48_48.jpg', cv2.IMREAD_GRAYSCALE )\n","    ##batch de uno\n","    #self.leo = leo.reshape((1, self.img_res[0], self.img_res[1], self.img_res[2]))\n","    #self.leo_lab = 6 * np.ones(1, 'int32' ) # 6 for neutral \n","    \n","    #when we use test data\n","    print(\"info de use_test_in_batch: \", self.use_test_in_batch)\n","    if self.use_test_in_batch:\n","      #revisar por que no esta el metodo leo_lab\n","      self.lab_vect_train = np.concatenate([self.lab_vect_train, self.lab_vect_test, self.leo_lab])\n","      self.img_vect_train = np.concatenate([self.img_vect_train, self.img_vect_test, self.leo])\n","\n","  def load_leo(self):\n","    \"\"\"Return label and image from reading\n","    \"\"\"\n","    return self.leo_lab , self.leo\n","              \n","  def load_data(self, domain=None, batch_size=1, is_testing=False, convertRGB=False):\n","  \n","    \"\"\"Load data function: load batch of data\n","\n","    Parameters\n","    ------------\n","    domain: int- class label \n","    batch_size: int- \n","    is_testing: boolean- test or not\n","    convertRGB: boolean- to make RGB images\n","\n","    Return\n","    ------------\n","    labels and images batch\n","    \"\"\"\n","    if is_testing:\n","      \n","      #when label class was not given\n","      if domain is None:\n","        idx = np.random.choice(self.img_vect_test.shape[0], size=batch_size)\n","      else:      \n","        assert domain in [0,1,2,3,4,5,6]# for check that label given is correct\n","        idx0 = np.argwhere(self.lab_vect_test == domain)#get shape of data with label to work \n","        idx1 = np.random.choice(idx0.shape[0], size=batch_size)#random choice\n","        idx = idx0[idx1]#from general data with the label we get the random data selected\n","        idx = np.squeeze(idx)#check size dimensions of idx***\n","      batch_images = self.img_vect_test[idx]\n","      labels = self.lab_vect_test[idx]\n","    #same for train data\n","    else:\n","      if domain is None:\n","        idx = np.random.choice(self.lab_vect_train.shape[0],size=batch_size)\n","      else:\n","        assert domain in [0,1,2,3,4,5,6]\n","        idx0 = np.argwhere(self.lab_vect_train == domain) \n","        idx1 = np.random.choice(idx0.shape[0],size=batch_size)\n","        idx = idx0[idx1]\n","        idx = np.squeeze(idx)\n","      batch_images = self.img_vect_train[idx]\n","      labels = self.lab_vect_train[idx]\n","        \n","    batch_images = np.resize(batch_images, (batch_size, self.img_res[0], self.img_res[1],\n","                            self.img_res[2]))\n","    \n","    if convertRGB:\n","      _batch_images = np.zeros((batch_size, self.img_res[0], self.img_res[1], 3))\n","      for i in range(batch_size):\n","        _batch_images[i] = cv2.cvtColor(batch_images[i], cv2.COLOR_GRAY2RGB)\n","      batch_images = _batch_images\n","    \n","    if is_testing:\n","      return labels , batch_images\n","    for i in range(batch_size):\n","      if np.random.random() > 0.5:#check its meaning***\n","        batch_images[i] = np.fliplr(batch_images[i]) #for column flip (its needed?***)\n","    return labels , batch_images\n","\n","  def load_batch(self, domain=None, batch_size=1, is_testing=False , convertRGB=False):\n","    \"\"\"\n","    Parameters:\n","    --------------\n","    domain: int- label class\n","    batch_size: int- amount of images to be treated\n","    is_testing: boolean- for testing pourposes\n","    convertRGB: boolean- for get RGB images\n","\n","    Return:\n","    --------------\n","    labels and their respective batch images\n","    \"\"\"\n","    if is_testing:\n","      raise Exception(\"not supported\")\n","    self.n_batches = int(len(self.img_vect_train) / batch_size)\n","    total_samples = self.n_batches * batch_size\n","    for i in range(self.n_batches):\n","      if domain is None:\n","        idx = np.random.choice(self.lab_vect_train.shape[0], size=batch_size)\n","      else:      \n","        assert domain in list(range(7))\n","        idx0 = np.argwhere(self.lab_vect_train == domain) \n","        idx1 = np.random.choice(idx0.shape[0], size=batch_size)\n","        idx = idx0[idx1]\n","        idx = np.squeeze(idx)\n","      batch_images = self.img_vect_train[idx]\n","      labels = self.lab_vect_train[idx]\n","      for i in range(batch_size):\n","        if np.random.random() > 0.5:#check its meaning***\n","          batch_images[i] = np.fliplr(batch_images[i]) #for column flip (its needed?***)\n","      batch_images = np.resize(batch_images, (batch_size,self.img_res[0],self.img_res[1],self.img_res[2]))\n","      if convertRGB:\n","        _batch_images = np.zeros((batch_size, self.img_res[0], self.img_res[1],3))\n","        for i in range(batch_size):\n","          _batch_images[i] = cv2.cvtColor(batch_images[i], cv2.COLOR_GRAY2RGB)\n","        batch_images = _batch_images\n","      yield labels , batch_images\n","\n","        #Nota: no entiendo muy bien la diferencia entre los metodos load_batch y load_data***\n","  \n","  \n","  def load_batch_AB(self, domain=None, batch_size=1, is_testing=False):\n","     \n","    \"\"\"Load batch of data from two domains (A and B)\n","    Parameters:\n","    ---------------\n","    domain: array- labels class\n","    batch_size: int- amount of data to be loaded\n","    is_testing: boolean- for testing pourposes\n","\n","    Return:\n","    ---------------\n","    Batch images from domains A and B\n","    Respective labels for data from both domains\n","    \"\"\"\n","    if is_testing:#it seems to no support testing (make is_testing=False always?***)\n","      raise Exception(\"not supported\")\n","    self.n_batches = int(len(self.img_vect_train) / batch_size)\n","    total_samples = self.n_batches * batch_size\n","    for i in range(self.n_batches):   \n","      assert domain is not None #check if domain is not empty \n","      assert type(domain) is list #domain type must be list format\n","      #check both domains belong to labels between [0,6] \n","      assert domain[0] in list(range(7))\n","      assert domain[1] in list(range(7))\n","      assert domain[0] != domain[1]#check different domains\n","      domain_A , domain_B = domain[0] , domain[1]\n","      # domain_A\n","      idx0 = np.argwhere(self.lab_vect_train == domain_A) \n","      idx1 = np.random.choice(idx0.shape[0],size=batch_size)\n","      idx = idx0[idx1]\n","      idx = np.squeeze(idx)\n","      batch_images_A = self.img_vect_train[idx]\n","      labels_A = self.lab_vect_train[idx]\n","      for i in range(batch_size):\n","        if np.random.random() > 10.5:#check its meaning***\n","          batch_images_A[i] = np.fliplr(batch_images_A[i])#for column flip (its needed?***)\n","      batch_images_A = np.resize(batch_images_A, (batch_size,self.img_res[0],self.img_res[1],self.img_res[2]))\n","      # domain_B\n","      idx0 = np.argwhere(self.lab_vect_train == domain_B) \n","      idx1 = np.random.choice(idx0.shape[0],size=batch_size)\n","      idx = idx0[idx1]\n","      idx = np.squeeze(idx)\n","      batch_images_B = self.img_vect_train[idx]\n","      labels_B = self.lab_vect_train[idx]\n","      for i in range(batch_size):\n","        if np.random.random() > 10.5:#check its meaning***\n","          batch_images_B[i] = np.fliplr(batch_images_B[i])#for column flip (its needed?***)\n","      batch_images_B = np.resize(batch_images_B, (batch_size,self.img_res[0],self.img_res[1],self.img_res[2]))\n","      \n","      yield labels_A , batch_images_A , labels_B , batch_images_B"],"metadata":{"id":"r3i3R3S8H54Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color='red'>**Models**</font>\n","## **Useful libraries**"],"metadata":{"id":"OILSfQzLMuF4"}},{"cell_type":"code","source":["!pip install git+https://www.github.com/keras-team/keras-contrib.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2b61mo28GyG4","executionInfo":{"status":"ok","timestamp":1643123768425,"user_tz":300,"elapsed":14168,"user":{"displayName":"Franklin Samuel Sierra Jerez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64","userId":"07062157042636671418"}},"outputId":"81e4cf6c-0e70-40bc-f7b5-96687406c07c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-2_uvr8cd\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-2_uvr8cd\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.7.0)\n"]}]},{"cell_type":"code","source":["from __future__ import print_function, division\n","import scipy\n","\n","from keras.datasets import mnist\n","from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n","from keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n","from keras.layers import Reshape\n","import datetime\n","import sys\n","from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n","import numpy as np\n","import os\n","import random \n","from keras.layers import Conv2DTranspose, BatchNormalization\n","import tensorflow as tf \n","\n","from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"vD5nmW7xNH8J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Useful methods"],"metadata":{"id":"4ZJrN2VnNf0g"}},{"cell_type":"code","source":["def get_dim_conv(dim,f,p,s):\n","  \"\"\"Function to calculate output conv shape\n","  Parameters:\n","  -----------\n","  dim: \n","  f: int- amount of filters\n","  p: int- padding\n","  s: int- stride \n","\n","  Return:\n","  -----------\n","  new output dimension\n","  \"\"\"\n","  return int((dim+2*p-f)/2+1)"],"metadata":{"id":"OJ7FjahnNUdp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_generator_enc_dec(img_shape, gf, num_classes, channels, num_layers=4, f_size=4, \n","                            tranform_layer=False):\n","  \"\"\"U-Net Generator\n","  Parameters:\n","  ------------\n","  img_shape: array- image shape\n","  gf: int- amount of filters check***\n","  num_classes: int- classes to be taken into account\n","  num_layers: int- number of layers in net\n","  f_size: int- filter size\n","  transform_layer: boolean- \n","\n","  Return: enconder and decoder nets\n","  \"\"\"\n","  \n","\n","  def conv2d(layer_input, filters, f_size=f_size):\n","    \"\"\"Layers used during downsampling\"\"\"\n","    d = Conv2D(filters, kernel_size=f_size, strides=2, padding='valid')(layer_input)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    d = InstanceNormalization()(d)\n","    return d    \n","\n","  #no se usa\n","  #def __deconv2d(layer_input, skip_input, filters, f_size=f_size, dropout_rate=0):\n","  #    \"\"\"Layers used during upsampling\"\"\"\n","  #    u = UpSampling2D(size=2)(layer_input)\n","  #    u = Conv2D(filters, kernel_size=f_size, strides=1, padding='valid', activation='relu')(u)\n","  #    if dropout_rate:\n","  #        u = Dropout(dropout_rate)(u)\n","  #    u = InstanceNormalization()(u)\n","  #    u = Concatenate()([u, skip_input])\n","  #    return u\n","  \n","  def deconv2d(layer_input, skip_input, filters, f_size=f_size, dropout_rate=0, output_padding=None):\n","  \n","    \"\"\"Layers used during upsampling\"\"\"\n","    u = Conv2DTranspose(filters=filters, kernel_size=f_size, strides=2, activation='relu',\n","                        output_padding=output_padding)(layer_input)\n","    if dropout_rate:\n","      u = Dropout(dropout_rate)(u)\n","    u = InstanceNormalization()(u)\n","    u = Concatenate()([u, skip_input])\n","    return u\n","\n","  # Image input layer\n","  img = Input(shape=img_shape)\n","\n","  # Downsampling\n","  d = img \n","  zs = [] \n","  dims = []\n","  _dim = img_shape[0]\n","  for i in range(num_layers):\n","    d = conv2d(d, gf*2**i)# add by 2 as we go deeper in the net\n","    zs.append(d)\n","    _dim = get_dim_conv(_dim,f_size,0,2)\n","    dims.append((_dim,gf*2**i))\n","    print(\"D:\",_dim,gf*2**i)\n","  G_enc = Model(img,zs)#encoder net\n","\n","  #### \n","  # = Input(shape=(24, 24, 32))\n","  #d2_ = Input(shape=(12, 12, 64))\n","  #d3_ = Input(shape=(6, 6, 128))\n","  #d4_ = Input(shape=(3, 3, 256))\n","  \n","  _zs = [] \n","  d_ , c_ = dims.pop()#check dims dim***\n","  print(0,d_,c_)\n","  i_ = Input(shape=(d_, d_, c_))\n","  _zs.append(i_)\n","  label = Input(shape=(num_classes,), dtype='float32')\n","  label_r = Reshape((1,1,num_classes))(label)\n","  \n","  u = concatenate([i_, label_r],axis=-1)\n","  \n","  ## transf (why?***)\n","  if tranform_layer:\n","    tr = Flatten()(u)\n","    tr = Dense(c_+num_classes)(tr)\n","    tr = LeakyReLU(alpha=0.2)(tr)\n","    u = Reshape((1,1,c_+num_classes))(tr)\n","  ##\n","\n","  u = Conv2D(c_, kernel_size=1, strides=1, padding='valid')(u) ## 1x1 conv \n","\n","  # Upsampling\n","  for i in range(num_layers-1):\n","    _ch = gf*2**((num_layers-2)-i)\n","    d_ , c_ = dims.pop()\n","    print(i,d_,c_)\n","    i_ = Input(shape=(d_, d_, c_))\n","    _zs.append(i_)\n","    if i == 2:\n","      u = deconv2d(u, i_, _ch, output_padding=1)\n","    else:\n","      u = deconv2d(u, i_, _ch)\n","      \n","  #u4 = UpSampling2D(size=2)(u)\n","  #output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n","  \n","  u = Conv2DTranspose(filters=channels, kernel_size=f_size, strides=2, activation='tanh', output_padding=None)(u)\n","  \n","  \n","  _zs.reverse()\n","  _zs.append(label)\n","  G_dec = Model(_zs,u) #decoder net\n","\n","  return G_enc , G_dec"],"metadata":{"id":"_nBAEgyrNptx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_discriminator(img_shape, df, num_classes, num_layers=4, act_multi_label='softmax'):\n","  \"\"\"Build discriminator function: net for discriminate real from fake data\n","  Parameters:\n","  -----------\n","  img_shape: array- (w,h,c)\n","  df: int- dimension filters check***\n","  num_layers: int- amount of model's layers\n","  act_multi_label: string- activation function\n","\n","  Return: discriminator model\n","  \"\"\"\n","\n","  def d_layer(layer_input, filters, f_size=4, normalization=True):\n","    \"\"\"Discriminator layer\"\"\"\n","    d = Conv2D(filters, kernel_size=f_size, strides=2, padding='valid')(layer_input)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    if normalization:\n","        d = InstanceNormalization()(d)\n","    return d\n","\n","  img = Input(shape=img_shape)\n","  \n","  #label = Input(shape=(1,), dtype='int32')\n","  #label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n","  #flat_img = Flatten()(img)\n","  #model_input = multiply([flat_img, label_embedding])\n","  #d0 = Reshape(self.img_shape)(model_input)\n","\n","  d = img \n","  for i in range(num_layers):\n","    #normalize all layers except the 1st one\n","    _norm = False if i == 0 else True \n","    d = d_layer(d, df*2**i, normalization=_norm)\n","\n","  flat_repr = Flatten()(d)#flat representation of the last layer\n","\n","  #validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n","\n","  print(\"flat_repr.get_shape().as_list():\",flat_repr.get_shape().as_list())\n","  print(\"flat_repr.get_shape().as_list()[1:]:\",flat_repr.get_shape().as_list()[1:])\n","\n","  #Dense neural net\n","  #Part to address the real or fake discrimination\n","  gan_logit = Dense(df*2**(num_layers-1))(flat_repr)\n","  gan_logit = LeakyReLU(alpha=0.2)(gan_logit)\n","  gan_prob = Dense(1, activation='sigmoid')(gan_logit)\n","\n","  #Part to address the class classification\n","  class_logit = Dense(df*2**(num_layers-1))(flat_repr)\n","  class_logit = LeakyReLU(alpha=0.2)(class_logit)\n","  class_prob = Dense(num_classes, activation=act_multi_label)(class_logit)\n","\n","  #### \n","  #label = Input(shape=(1,), dtype='int32')\n","  #label_embedding = Flatten()(Embedding(self.num_classes, 9)(label))\n","  #flat_img = Flatten()(validity)\n","  #d44 = multiply([flat_img, label_embedding])\n","  #d444 = Reshape(validity.get_shape().as_list()[1:])(d44)\n","  ####\n","\n","  return Model(img, [gan_prob, class_prob])"],"metadata":{"id":"wZdAnRmIOaaL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color='red'>**Conditional cycleGan network**</font>\n","## **Useful libraries**"],"metadata":{"id":"GyjrNuOyk_P9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYDBsu4lXiT8"},"outputs":[],"source":["from __future__ import print_function, division\n","import scipy\n","\n","from tensorflow.keras.datasets import mnist\n","from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n","from keras.models import Sequential, Model\n","#from keras.optimizers import Adam\n","from tensorflow.keras.optimizers import Adam\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n","from keras.layers import Reshape\n","import datetime\n","import matplotlib.pyplot as plt\n","import sys\n","#from data_loader import DataLoader\n","from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n","import numpy as np\n","import pandas as pd \n","import os\n","import random \n","\n","import tensorflow as tf \n","\n","from tensorflow.keras.utils import to_categorical\n","import argparse\n","from sklearn.metrics import accuracy_score\n","\n","#from  models import *"]},{"cell_type":"code","source":["class CCycleGAN():\n","  \"\"\" Conditional cycleGan: model initialization (generator and discriminator nets) and training,\n","  receive image shape,\n","  amount of classes to be taken into account, \n","  weight losses for generator and discriminator nets,\n","  load the dataset.\n","\n","  Parameters\n","  ------------\n","  img_rows and img_cols: int- rows and cols for image to work with\n","  channels: int- amount of image channels\n","  num_classes: int- amount of classes to be taken into account\n","  d_gan_loss_w: int- discriminator loss weight\n","  d_cl_loss_w: int- discriminator loss weight for class tag\n","  g_gan_loss_w: int- generator loss weight\n","  g_cl_loss_w: int- generator loss weight for class tag\n","  ---> rec_loss_w: int- cycle consistency loss weight (check)\n","  adam_lr: float- learning rate\n","  adam_beta_1: float- parameters for adam rule\n","  adam_beta_2: float- parameters for adam rule \n","  \"\"\"\n","\n","  #values assignment\n","  def __init__(self,img_rows = 48,img_cols = 48,channels = 1, num_classes=7, d_gan_loss_w=1,\n","      d_cl_loss_w=1, g_gan_loss_w=1, g_cl_loss_w=1, rec_loss_w=1, adam_lr=0.0002, adam_beta_1=0.5,\n","      adam_beta_2=0.999):\n","  \n","    # Input shape\n","    self.img_rows = img_rows\n","    self.img_cols = img_cols\n","    self.channels = channels\n","    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","    self.num_classes = num_classes\n","\n","    # Loss weights \n","    self.d_gan_loss_w = d_gan_loss_w\n","    self.d_cl_loss_w = d_cl_loss_w\n","    self.g_gan_loss_w = g_gan_loss_w\n","    self.g_cl_loss_w = g_cl_loss_w\n","    self.rec_loss_w = rec_loss_w\n","\n","    # optmizer params \n","    self.adam_lr = adam_lr\n","    self.adam_beta_1 = adam_beta_1\n","    self.adam_beta_2 = adam_beta_2\n","\n","    # Configure data loader\n","    self.dataset_name = 'fer2013' #maybe changed dataset name (ver importancia del nombre?***)\n","    # TALE SECOND PART (pasar a file: data_loader.py)\n","    self.data_loader = DataLoader(dataset_name=self.dataset_name,img_res=self.img_shape,use_test_in_batch=False)\n","    # label dict\n","    self.lab_dict = {0: \"Angry\", 1: \"Disgust\" , 2: \"Fear\" , 3: \"Happy\" , 4: \"Sad\" , 5: \"Surprise\" , 6: \"Neutral\"}\n","\n","    # Number of filters in the first layer of Generator and Discriminator\n","    self.gf = 32\n","    self.df = 64\n","\n","    optimizer = Adam(self.adam_lr, self.adam_beta_1, self.adam_beta_2) \n","\n","    # Build and compile the discriminators (models.py method)\n","    self.d = build_discriminator(img_shape=self.img_shape, df=64, num_classes=self.num_classes,\n","                                act_multi_label='sigmoid')\n","    print(\"******** Discriminator/Classifier ********\")\n","    self.d.summary()\n","    self.d.compile(loss=[\n","        'binary_crossentropy',  # gan\n","        'binary_crossentropy'   # class \n","        ],\n","        optimizer=optimizer,\n","        metrics=['accuracy'],\n","        loss_weights=[\n","        self.d_gan_loss_w , # gan\n","        self.d_cl_loss_w   # class\n","        ])\n","\n","    #-------------------------\n","    # Construct Computational\n","    #   Graph of Generators\n","    #-------------------------\n","\n","    # Build the generators (here i go#1)\n","    self.g_enc , self.g_dec = build_generator_enc_dec(img_shape=(48,48,1), gf=64, num_classes=7, channels=1, tranform_layer=True)\n","    print(\"******** Generator_ENC ********\")\n","    self.g_enc.summary()\n","    print(\"******** Generator_DEC ********\")\n","    self.g_dec.summary()\n","\n","    # Input images from both domains\n","    img = Input(shape=self.img_shape)\n","    label0 = Input(shape=(self.num_classes,))\n","    label1 = Input(shape=(self.num_classes,))\n","\n","    # Translate images to the other domain\n","    z1,z2,z3,z4 = self.g_enc(img)\n","    fake = self.g_dec([z1,z2,z3,z4,label1])\n","\n","    # Translate images back to original domain\n","    reconstr = self.g_dec([z1,z2,z3,z4,label0])\n","\n","    # For the combined model we will only train the generators\n","    self.d.trainable = False\n","\n","    # Discriminators determines validity of translated images gan_prob,\n","    # class_prob [label,img], [gan_prob,class_prob]\n","    gan_valid , class_valid = self.d(fake)\n","\n","    # Combined model trains generators to fool discriminators\n","    self.combined = Model(inputs=[img,label0,label1], outputs=[ gan_valid, class_valid, reconstr])\n","    self.combined.compile(loss=['binary_crossentropy','categorical_crossentropy',\n","                                'mae'],\n","                          loss_weights=[                                            \n","                                        self.g_gan_loss_w, # g_loss gan \n","                                        self.g_cl_loss_w, # g_loss class  \n","                                        self.rec_loss_w # reconstruction loss\n","                                      ],\n","                        optimizer=optimizer)\n","\n","    print(\"******** Combined model ********\")\n","    self.combined.summary()\n","  \n","  def generate_new_labels(self,labels0):\n","    labels1 = [] \n","    for i in range(len(labels0)):\n","      allowed_values = list(range(0, self.num_classes))\n","      allowed_values.remove(labels0[i])\n","      labels1.append(random.choice(allowed_values))\n","    return np.array(labels1,'int32')\n","  \n","  def generate_new_labels_all(self, labels0):\n","  \n","    #called from training procedure check***\n","    \"\"\"Function for keep label values different from original labels\n","    Parameter:\n","    labels0: array- real label class list\n","    Return: array with all labels different from original label class\n","    \"\"\"\n","    labels_all = [] \n","    for i in range(len(labels0)):\n","      allowed_values = list(range(0, self.num_classes))\n","      allowed_values.remove(labels0[i])\n","      labels_all.append(np.array(allowed_values,'int32'))\n","    return np.array(labels_all,'int32')\n","\n","  def train(self, epochs, batch_size=1, sample_interval=50 , d_g_ratio=5): \n","    \"\"\"Conditional cycleGan training function\n","    Parameters:\n","    ------------\n","    epochs: int- amount of epochs to train model\n","    batch_size: int- number of samples to be taken into account for each update step\n","    sample_interval: int- check***\n","    d_g_ratio: int- epoch frequency for decay learning rate check***\n","    \"\"\"\n","\n","    start_time = datetime.datetime.now()\n","    # logs \n","    epoch_history, batch_i_history,  = [] , []   \n","    d_gan_loss_history, d_gan_accuracy_history, d_cl_loss_history, d_cl_accuracy_history = [], [], [], [] \n","    g_gan_loss_history, g_cl_loss_history = [] , [] \n","    reconstr_history = [] \n","\n","    # Adversarial loss ground truths\n","    valid = np.ones((batch_size,1) )\n","    fake = np.zeros((batch_size,1) )\n","\n","    null_labels = np.zeros((batch_size,7) )\n","\n","    for epoch in range(epochs):\n","      for batch_i, (labels0 , imgs) in enumerate(self.data_loader.load_batch(batch_size=batch_size)):\n","        labels1_all = self.generate_new_labels_all(labels0)\n","\n","        labels0_cat = to_categorical(labels0, num_classes=self.num_classes)\n","        #\n","        labels1_all_1 = to_categorical(labels1_all[:,0], num_classes=self.num_classes)\n","        labels1_all_2 = to_categorical(labels1_all[:,1], num_classes=self.num_classes)\n","        labels1_all_3 = to_categorical(labels1_all[:,2], num_classes=self.num_classes)\n","        labels1_all_4 = to_categorical(labels1_all[:,3], num_classes=self.num_classes)\n","        labels1_all_5 = to_categorical(labels1_all[:,4], num_classes=self.num_classes)\n","        labels1_all_6 = to_categorical(labels1_all[:,5], num_classes=self.num_classes)\n","        \n","        # ----------------------\n","        #  Train Discriminators\n","        # ----------------------\n","\n","        # Translate images to opposite domain\n","        zs1,zs2,zs3,zs4 = self.g_enc.predict(imgs)#check what encoder returns***\n","        fakes_1 = self.g_dec.predict([zs1,zs2,zs3,zs4,labels1_all_1])\n","        fakes_2 = self.g_dec.predict([zs1,zs2,zs3,zs4,labels1_all_2])\n","        fakes_3 = self.g_dec.predict([zs1,zs2,zs3,zs4,labels1_all_3])\n","        fakes_4 = self.g_dec.predict([zs1,zs2,zs3,zs4,labels1_all_4])\n","        fakes_5 = self.g_dec.predict([zs1,zs2,zs3,zs4,labels1_all_5])\n","        fakes_6 = self.g_dec.predict([zs1,zs2,zs3,zs4,labels1_all_6])\n","\n","        # Train the discriminators (original images = real / translated = Fake)\n","        idx = np.random.permutation(self.num_classes*labels0.shape[0])\n","        _labels_cat = np.concatenate([labels0_cat,\n","                                      null_labels,\n","                                      null_labels,\n","                                      null_labels,\n","                                      null_labels,\n","                                      null_labels,\n","                                      null_labels])\n","        _imgs = np.concatenate([imgs,\n","                                fakes_1,\n","                                fakes_2,\n","                                fakes_3,\n","                                fakes_4,\n","                                fakes_5,\n","                                fakes_6])\n","        _vf = np.concatenate([valid, fake, fake, fake, fake, fake, fake])\n","        _labels_cat = _labels_cat[idx]\n","        _imgs = _imgs[idx]\n","        _vf = _vf[idx]\n","\n","        d_loss  = self.d.train_on_batch(_imgs, [_vf,_labels_cat])\n","\n","        if batch_i % d_g_ratio == 0: \n","\n","          # ------------------\n","          #  Train Generators\n","          # ------------------\n","          _imgs = np.concatenate([imgs,                                                     \n","                                  imgs,\n","                                  imgs,\n","                                  imgs,\n","                                  imgs,\n","                                  imgs])\n","\n","          _labels0_cat = np.concatenate([labels0_cat,                                                               \n","                                        labels0_cat,\n","                                        labels0_cat,\n","                                        labels0_cat,\n","                                        labels0_cat,\n","                                        labels0_cat])\n","\n","          _labels1_all_other = np.concatenate([labels1_all_1,                                                                                \n","                                              labels1_all_2,\n","                                              labels1_all_3,\n","                                              labels1_all_4,\n","                                              labels1_all_5,\n","                                              labels1_all_6])\n","\n","          # I know this should be outside the loop;\n","          # left here to make code more understandable \n","          _valid = np.concatenate([valid,                                                      \n","                                  valid,\n","                                  valid,\n","                                  valid,\n","                                  valid,\n","                                  valid])\n","\n","          idx = np.random.permutation((self.num_classes-1)*labels0.shape[0])\n","          _imgs = _imgs[idx]\n","          _labels0_cat = _labels0_cat[idx]\n","          _labels1_all_other = _labels1_all_other[idx]\n","          _valid = _valid[idx]\n","\n","          # Train the generators\n","          g_loss = self.combined.train_on_batch([_imgs, _labels0_cat, _labels1_all_other],[_valid, _labels1_all_other, _imgs])\n","\n","          elapsed_time = datetime.datetime.now() - start_time\n","\n","          print (\"[Epoch %d/%d] [Batch %d/%d] [D_gan loss: %f, acc_gan: %3d%%] [D_cl loss: %f, acc_cl: %3d%%] [G_gan loss: %05f, G_cl: %05f, recon: %05f] time: %s \" \\\n","              % ( epoch, epochs,\n","                  batch_i, self.data_loader.n_batches,\n","                  d_loss[1],100*d_loss[3],d_loss[2],100*d_loss[4],\n","                  g_loss[1],g_loss[2],g_loss[3],\n","                  elapsed_time))\n","\n","          # log\n","          epoch_history.append(epoch) \n","          batch_i_history.append(batch_i)\n","          d_gan_loss_history.append(d_loss[1])\n","          d_gan_accuracy_history.append(100*d_loss[3])\n","          d_cl_loss_history.append(d_loss[2])\n","          d_cl_accuracy_history.append(100*d_loss[4])\n","          g_gan_loss_history.append(g_loss[1])\n","          g_cl_loss_history.append(g_loss[2])\n","          reconstr_history.append(g_loss[3])\n","\n","        # If at save interval => save generated image samples\n","        if batch_i % sample_interval == 0:    \n","          self.sample_images(epoch, batch_i)\n","          #self.sample_images(epoch, batch_i,use_leo=True)\n","\n","          train_history = pd.DataFrame({\n","              'epoch': epoch_history, \n","              'batch': batch_i_history, \n","              'd_gan_loss': d_gan_loss_history, \n","              'd_gan_accuracy' : d_gan_accuracy_history,\n","              'd_cl_loss': d_cl_loss_history, \n","              'd_cl_accuracy': d_cl_accuracy_history, \n","              'g_gan_loss': g_gan_loss_history, \n","              'g_cl_loss': g_cl_loss_history, \n","              'reconstr_loss': reconstr_history\n","          })\n","          train_history.to_csv(str(sys.argv[0]).split('.')[0]+'_train_log.csv',index=False)\n","\n","  def sample_images(self, epoch, batch_i, use_leo=False):\n","  \n","    \"\"\"Function to save a batch test samples\n","    Parameters:\n","    ------------\n","    epoch: int- epoch where the interval save is done\n","    batch_i: int- number of batch where we want to save\n","    \"\"\"\n","    ## disc\n","    labels0_d , imgs_d = self.data_loader.load_data(batch_size=64, is_testing=True)\n","    #predicting images with discriminator net\n","    gan_pred_prob, class_pred_prob = self.d.predict(imgs_d)\n","    \n","    gan_pred = (gan_pred_prob > 0.5)*1.0\n","    gan_pred = gan_pred.reshape((64,))\n","    \n","    class_pred = np.argmax(class_pred_prob,axis=1)\n","\n","    gan_test_accuracy = accuracy_score(y_true=np.ones(64), y_pred=gan_pred)\n","    class_test_accuracy = accuracy_score(y_true=labels0_d, y_pred=class_pred)\n","\n","    print(\"*** TEST *** [D_gan accuracy :\",gan_test_accuracy,\"] [D_cl accuracy :\", class_test_accuracy,\"]\")\n","\n","    ## gen         \n","    if use_leo:\n","      labels0_ , imgs_ = self.data_loader.load_leo()#load_leo() why?***\n","    else:\n","      labels0_ , imgs_ = self.data_loader.load_data(batch_size=1, is_testing=True)\n","    labels1_all = self.generate_new_labels_all(labels0_)\n","\n","    labels0_cat = to_categorical(labels0_, num_classes=self.num_classes)\n","    labels1_all_1 = to_categorical(labels1_all[:,0], num_classes=self.num_classes)\n","    labels1_all_2 = to_categorical(labels1_all[:,1], num_classes=self.num_classes)\n","    labels1_all_3 = to_categorical(labels1_all[:,2], num_classes=self.num_classes)\n","    labels1_all_4 = to_categorical(labels1_all[:,3], num_classes=self.num_classes)\n","    labels1_all_5 = to_categorical(labels1_all[:,4], num_classes=self.num_classes)\n","    labels1_all_6 = to_categorical(labels1_all[:,5], num_classes=self.num_classes)\n","\n","    # Translate images \n","    zs1_,zs2_,zs3_,zs4_ = self.g_enc.predict(imgs_)\n","    fake_1 = self.g_dec.predict([zs1_,zs2_,zs3_,zs4_,labels1_all_1])\n","    fake_2 = self.g_dec.predict([zs1_,zs2_,zs3_,zs4_,labels1_all_2])\n","    fake_3 = self.g_dec.predict([zs1_,zs2_,zs3_,zs4_,labels1_all_3])\n","    fake_4 = self.g_dec.predict([zs1_,zs2_,zs3_,zs4_,labels1_all_4])\n","    fake_5 = self.g_dec.predict([zs1_,zs2_,zs3_,zs4_,labels1_all_5])\n","    fake_6 = self.g_dec.predict([zs1_,zs2_,zs3_,zs4_,labels1_all_6])\n","  \n","    # Reconstruct image \n","    reconstr_ = self.g_dec.predict([zs1_,zs2_,zs3_,zs4_,labels0_cat])\n","\n","    gen_imgs = np.concatenate([imgs_,                              \n","                              fake_1, \n","                              fake_2, \n","                              fake_3, \n","                              fake_4, \n","                              fake_5, \n","                              fake_6,\n","                              reconstr_])\n","\n","    # Rescale images 0 - 1\n","    gen_imgs = 0.5 * gen_imgs + 0.5# check the rol of 0.5***\n","\n","    titles = ['Orig:'+str(self.lab_dict[labels0_.item(0)]), \n","              'Trans:'+str(self.lab_dict[labels1_all[:,0].item(0)]),\n","              'Trans:'+str(self.lab_dict[labels1_all[:,1].item(0)]),\n","              'Trans:'+str(self.lab_dict[labels1_all[:,2].item(0)]),\n","              'Trans:'+str(self.lab_dict[labels1_all[:,3].item(0)]),\n","              'Trans:'+str(self.lab_dict[labels1_all[:,4].item(0)]),\n","              'Trans:'+str(self.lab_dict[labels1_all[:,5].item(0)]),\n","              'Reconstr.']\n","    r, c = 2, 4#for rows and cols\n","    fig, axs = plt.subplots(r, c)\n","    \n","    plt.subplots_adjust(hspace=0)\n","\n","    if not os.path.exists( \"images/%s/\"% (self.dataset_name)):\n","      os.makedirs( \"images/%s/\"% (self.dataset_name)  )\n","    \n","    cnt = 0\n","    for i in range(r):\n","      for j in range(c):\n","        axs[i,j].imshow(gen_imgs[cnt].reshape((self.img_rows,self.img_cols)),cmap='gray')\n","        axs[i,j].set_title(titles[cnt])\n","        axs[i,j].axis('off')\n","        cnt += 1\n","\n","    if use_leo:\n","      fig.savefig(\"images/%s/%d_%d_leo.png\" % (self.dataset_name, epoch, batch_i))\n","    else:\n","      fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n","    plt.close()"],"metadata":{"id":"AxHvJPvFGu0e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Second idea**"],"metadata":{"id":"5NHqVz1JemJy"}},{"cell_type":"code","source":["class CCycleGAN():\n","\n","  def __init__(self, img_rows=48, img_cols=48, channels=1, num_classes=7, latent_dim=99, \n","               PREFIX='saved_model/'):\n","    # Input shape\n","    self.img_rows = img_rows\n","    self.img_cols = img_cols\n","    self.channels = channels\n","    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","    self.num_classes = num_classes\n","    self.latent_dim = latent_dim\n","    self.PREFIX=PREFIX\n","    \n","    ## dict\n","    self.lab_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n","\n","    # Configure data loader\n","    self.dataset_name = 'fer2013'\n","    self.data_loader = DataLoader(dataset_name=self.dataset_name,img_res=self.img_shape)\n","\n","    optimizer = Adam(0.0002, 0.5)\n","\n","    # Build and compile the discriminators\n","    self.d = self.build_discriminator2()\n","    print(\"******** Discriminator ********\")\n","    self.d.summary()\n","    self.d.compile(loss='categorical_crossentropy',\n","                   optimizer=optimizer,\n","                   metrics=['accuracy'])\n","  \n","  def build_discriminator2(self):    \n","    base_model  = ResNet50(weights= 'imagenet', include_top=False, input_shape= (48,48,3))\n","    \n","    # add a global spatial average pooling layer\n","    x = base_model.output\n","    latent_repr = GlobalAveragePooling2D()(x)\n","    \n","    # let's add a fully-connected layer\n","    f = Dense(1024, activation='relu')(latent_repr)\n","    predictions = Dense(self.num_classes, activation='softmax')(f)\n","    \n","    return Model(base_model.input, predictions)\n","\n","  def train(self, epochs, batch_size=1, sample_interval=50):\n","    start_time = datetime.datetime.now()\n","\n","    # Adversarial loss ground truths\n","    valid = np.ones((batch_size,1))\n","    fake = np.zeros((batch_size,1))\n","    \n","    earlystopper = EarlyStopping(patience=20, verbose=1,monitor='val_acc',mode='max')\n","    checkpointer = ModelCheckpoint(self.PREFIX+'classifier_2.h5', verbose=1, save_best_only=True,monitor='val_acc',mode='max')\n","    reduce_lr = ReduceLROnPlateau(factor=0.2, patience=5, min_lr=0.00001, verbose=1,monitor='val_acc',mode='max')\n","    results = self.d.fit(self.data_loader.img_vect_train_RGB, \n","                        np_utils.to_categorical(self.data_loader.lab_vect_train,num_classes=self.num_classes),\n","                validation_data=[self.data_loader.img_vect_test_RGB,\n","                                  np_utils.to_categorical(self.data_loader.lab_vect_test,num_classes=self.num_classes)],\n","                batch_size=batch_size, epochs=epochs,\n","                callbacks=[earlystopper, checkpointer,reduce_lr], shuffle=True)"],"metadata":{"id":"yVWj_zG7eUUW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color='red'>Main</font>"],"metadata":{"id":"RPPr9fC3TaL4"}},{"cell_type":"code","source":["d_gan_loss_w =1\n","d_cl_loss_w =1\n","g_gan_loss_w =2\n","g_cl_loss_w =2\n","rec_loss_w =1\n","adam_lr =0.0002\n","adam_beta_1 =0.5\n","adam_beta_2 =0.999\n","epochs =170\n","batch_size =64\n","sample_interval =200"],"metadata":{"id":"e5c-ISS1TEFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CCycleGAN: THE TALE START HERE\n","gan = CCycleGAN(d_gan_loss_w=d_gan_loss_w, d_cl_loss_w=d_cl_loss_w,            \n","                g_gan_loss_w=g_gan_loss_w, g_cl_loss_w=g_cl_loss_w,\n","                rec_loss_w=rec_loss_w, adam_lr=adam_lr,\n","                adam_beta_1=adam_beta_1, adam_beta_2=adam_beta_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUZw1qOLUsZz","executionInfo":{"status":"ok","timestamp":1643123813966,"user_tz":300,"elapsed":37240,"user":{"displayName":"Franklin Samuel Sierra Jerez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64","userId":"07062157042636671418"}},"outputId":"807f9191-0837-449e-c3c9-874c4b77a2a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[">> loading fer2013 ...\n","passing throught all data...\n","> loaded train: 28709    - test: 7178\n","converting img to RGB...\n","ready test RGB!\n","ready train RGB!\n","info de use_test_in_batch:  False\n","flat_repr.get_shape().as_list(): [None, 512]\n","flat_repr.get_shape().as_list()[1:]: [512]\n","******** Discriminator/Classifier ********\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 48, 48, 1)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 23, 23, 64)   1088        ['input_1[0][0]']                \n","                                                                                                  \n"," leaky_re_lu (LeakyReLU)        (None, 23, 23, 64)   0           ['conv2d[0][0]']                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 10, 10, 128)  131200      ['leaky_re_lu[0][0]']            \n","                                                                                                  \n"," leaky_re_lu_1 (LeakyReLU)      (None, 10, 10, 128)  0           ['conv2d_1[0][0]']               \n","                                                                                                  \n"," instance_normalization (Instan  (None, 10, 10, 128)  2          ['leaky_re_lu_1[0][0]']          \n"," ceNormalization)                                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 4, 4, 256)    524544      ['instance_normalization[0][0]'] \n","                                                                                                  \n"," leaky_re_lu_2 (LeakyReLU)      (None, 4, 4, 256)    0           ['conv2d_2[0][0]']               \n","                                                                                                  \n"," instance_normalization_1 (Inst  (None, 4, 4, 256)   2           ['leaky_re_lu_2[0][0]']          \n"," anceNormalization)                                                                               \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 1, 1, 512)    2097664     ['instance_normalization_1[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," leaky_re_lu_3 (LeakyReLU)      (None, 1, 1, 512)    0           ['conv2d_3[0][0]']               \n","                                                                                                  \n"," instance_normalization_2 (Inst  (None, 1, 1, 512)   2           ['leaky_re_lu_3[0][0]']          \n"," anceNormalization)                                                                               \n","                                                                                                  \n"," flatten (Flatten)              (None, 512)          0           ['instance_normalization_2[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          262656      ['flatten[0][0]']                \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          262656      ['flatten[0][0]']                \n","                                                                                                  \n"," leaky_re_lu_4 (LeakyReLU)      (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," leaky_re_lu_5 (LeakyReLU)      (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_1 (Dense)                (None, 1)            513         ['leaky_re_lu_4[0][0]']          \n","                                                                                                  \n"," dense_3 (Dense)                (None, 7)            3591        ['leaky_re_lu_5[0][0]']          \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,283,918\n","Trainable params: 3,283,918\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","D: 23 64\n","D: 10 128\n","D: 4 256\n","D: 1 512\n","0 1 512\n","0 4 256\n","1 10 128\n","2 23 64\n","******** Generator_ENC ********\n","Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 48, 48, 1)]       0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 23, 23, 64)        1088      \n","                                                                 \n"," leaky_re_lu_6 (LeakyReLU)   (None, 23, 23, 64)        0         \n","                                                                 \n"," instance_normalization_3 (I  (None, 23, 23, 64)       2         \n"," nstanceNormalization)                                           \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 10, 10, 128)       131200    \n","                                                                 \n"," leaky_re_lu_7 (LeakyReLU)   (None, 10, 10, 128)       0         \n","                                                                 \n"," instance_normalization_4 (I  (None, 10, 10, 128)      2         \n"," nstanceNormalization)                                           \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 4, 4, 256)         524544    \n","                                                                 \n"," leaky_re_lu_8 (LeakyReLU)   (None, 4, 4, 256)         0         \n","                                                                 \n"," instance_normalization_5 (I  (None, 4, 4, 256)        2         \n"," nstanceNormalization)                                           \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 1, 1, 512)         2097664   \n","                                                                 \n"," leaky_re_lu_9 (LeakyReLU)   (None, 1, 1, 512)         0         \n","                                                                 \n"," instance_normalization_6 (I  (None, 1, 1, 512)        2         \n"," nstanceNormalization)                                           \n","                                                                 \n","=================================================================\n","Total params: 2,754,504\n","Trainable params: 2,754,504\n","Non-trainable params: 0\n","_________________________________________________________________\n","******** Generator_DEC ********\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_4 (InputLayer)           [(None, 7)]          0           []                               \n","                                                                                                  \n"," input_3 (InputLayer)           [(None, 1, 1, 512)]  0           []                               \n","                                                                                                  \n"," reshape (Reshape)              (None, 1, 1, 7)      0           ['input_4[0][0]']                \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 1, 1, 519)    0           ['input_3[0][0]',                \n","                                                                  'reshape[0][0]']                \n","                                                                                                  \n"," flatten_1 (Flatten)            (None, 519)          0           ['concatenate[0][0]']            \n","                                                                                                  \n"," dense_4 (Dense)                (None, 519)          269880      ['flatten_1[0][0]']              \n","                                                                                                  \n"," leaky_re_lu_10 (LeakyReLU)     (None, 519)          0           ['dense_4[0][0]']                \n","                                                                                                  \n"," reshape_1 (Reshape)            (None, 1, 1, 519)    0           ['leaky_re_lu_10[0][0]']         \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 1, 1, 512)    266240      ['reshape_1[0][0]']              \n","                                                                                                  \n"," conv2d_transpose (Conv2DTransp  (None, 4, 4, 256)   2097408     ['conv2d_8[0][0]']               \n"," ose)                                                                                             \n","                                                                                                  \n"," instance_normalization_7 (Inst  (None, 4, 4, 256)   2           ['conv2d_transpose[0][0]']       \n"," anceNormalization)                                                                               \n","                                                                                                  \n"," input_5 (InputLayer)           [(None, 4, 4, 256)]  0           []                               \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 4, 4, 512)    0           ['instance_normalization_7[0][0]'\n","                                                                 , 'input_5[0][0]']               \n","                                                                                                  \n"," conv2d_transpose_1 (Conv2DTran  (None, 10, 10, 128)  1048704    ['concatenate_1[0][0]']          \n"," spose)                                                                                           \n","                                                                                                  \n"," instance_normalization_8 (Inst  (None, 10, 10, 128)  2          ['conv2d_transpose_1[0][0]']     \n"," anceNormalization)                                                                               \n","                                                                                                  \n"," input_6 (InputLayer)           [(None, 10, 10, 128  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 10, 10, 256)  0           ['instance_normalization_8[0][0]'\n","                                                                 , 'input_6[0][0]']               \n","                                                                                                  \n"," conv2d_transpose_2 (Conv2DTran  (None, 23, 23, 64)  262208      ['concatenate_2[0][0]']          \n"," spose)                                                                                           \n","                                                                                                  \n"," instance_normalization_9 (Inst  (None, 23, 23, 64)  2           ['conv2d_transpose_2[0][0]']     \n"," anceNormalization)                                                                               \n","                                                                                                  \n"," input_7 (InputLayer)           [(None, 23, 23, 64)  0           []                               \n","                                ]                                                                 \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 23, 23, 128)  0           ['instance_normalization_9[0][0]'\n","                                                                 , 'input_7[0][0]']               \n","                                                                                                  \n"," conv2d_transpose_3 (Conv2DTran  (None, 48, 48, 1)   2049        ['concatenate_3[0][0]']          \n"," spose)                                                                                           \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,946,495\n","Trainable params: 3,946,495\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","******** Combined model ********\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_8 (InputLayer)           [(None, 48, 48, 1)]  0           []                               \n","                                                                                                  \n"," model_1 (Functional)           [(None, 23, 23, 64)  2754504     ['input_8[0][0]']                \n","                                , (None, 10, 10, 12                                               \n","                                8),                                                               \n","                                 (None, 4, 4, 256),                                               \n","                                 (None, 1, 1, 512)]                                               \n","                                                                                                  \n"," input_10 (InputLayer)          [(None, 7)]          0           []                               \n","                                                                                                  \n"," model_2 (Functional)           (None, 48, 48, 1)    3946495     ['model_1[0][0]',                \n","                                                                  'model_1[0][1]',                \n","                                                                  'model_1[0][2]',                \n","                                                                  'model_1[0][3]',                \n","                                                                  'input_10[0][0]',               \n","                                                                  'model_1[0][0]',                \n","                                                                  'model_1[0][1]',                \n","                                                                  'model_1[0][2]',                \n","                                                                  'model_1[0][3]',                \n","                                                                  'input_9[0][0]']                \n","                                                                                                  \n"," input_9 (InputLayer)           [(None, 7)]          0           []                               \n","                                                                                                  \n"," model (Functional)             [(None, 1),          3283918     ['model_2[0][0]']                \n","                                 (None, 7)]                                                       \n","                                                                                                  \n","==================================================================================================\n","Total params: 9,984,917\n","Trainable params: 6,700,999\n","Non-trainable params: 3,283,918\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["gan.train(epochs=epochs, batch_size=batch_size, sample_interval=sample_interval)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gXVL0y9w46S","outputId":"03f88c16-62a9-4b95-d93b-0e60c10d6cb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 0/170] [Batch 0/448] [D_gan loss: 1.099340, acc_gan:  27%] [D_cl loss: 0.894445, acc_cl:  12%] [G_gan loss: 6.676730, G_cl: 2.211193, recon: 0.619851] time: 0:00:30.329775 \n","*** TEST *** [D_gan accuracy : 0.453125 ] [D_cl accuracy : 0.140625 ]\n","[Epoch 0/170] [Batch 5/448] [D_gan loss: 0.017822, acc_gan: 100%] [D_cl loss: 0.069919, acc_cl:   8%] [G_gan loss: 5.101804, G_cl: 2.125021, recon: 0.591838] time: 0:01:30.889988 \n","[Epoch 0/170] [Batch 10/448] [D_gan loss: 0.180467, acc_gan:  93%] [D_cl loss: 0.084440, acc_cl:   4%] [G_gan loss: 4.304717, G_cl: 2.103636, recon: 0.520898] time: 0:02:28.070838 \n","[Epoch 0/170] [Batch 15/448] [D_gan loss: 0.115124, acc_gan:  94%] [D_cl loss: 0.076889, acc_cl:   5%] [G_gan loss: 3.161157, G_cl: 2.080728, recon: 0.481476] time: 0:03:28.363809 \n","[Epoch 0/170] [Batch 20/448] [D_gan loss: 0.296064, acc_gan:  89%] [D_cl loss: 0.088204, acc_cl:   2%] [G_gan loss: 2.901002, G_cl: 2.239566, recon: 0.418395] time: 0:04:29.108162 \n","[Epoch 0/170] [Batch 25/448] [D_gan loss: 0.309279, acc_gan:  87%] [D_cl loss: 0.092323, acc_cl:   4%] [G_gan loss: 2.159837, G_cl: 2.231250, recon: 0.380860] time: 0:05:26.238503 \n","[Epoch 0/170] [Batch 30/448] [D_gan loss: 0.297589, acc_gan:  87%] [D_cl loss: 0.093959, acc_cl:  16%] [G_gan loss: 2.264498, G_cl: 2.218734, recon: 0.390023] time: 0:06:25.827411 \n","[Epoch 0/170] [Batch 35/448] [D_gan loss: 0.256973, acc_gan:  89%] [D_cl loss: 0.089948, acc_cl:   8%] [G_gan loss: 2.686877, G_cl: 2.249045, recon: 0.379260] time: 0:07:26.345186 \n","[Epoch 0/170] [Batch 40/448] [D_gan loss: 0.194623, acc_gan:  92%] [D_cl loss: 0.087675, acc_cl:   6%] [G_gan loss: 3.112279, G_cl: 2.204099, recon: 0.375869] time: 0:08:24.592000 \n","[Epoch 0/170] [Batch 45/448] [D_gan loss: 0.219633, acc_gan:  93%] [D_cl loss: 0.085818, acc_cl:   2%] [G_gan loss: 4.071711, G_cl: 2.128707, recon: 0.385549] time: 0:09:22.983803 \n","[Epoch 0/170] [Batch 50/448] [D_gan loss: 0.202481, acc_gan:  91%] [D_cl loss: 0.082431, acc_cl:  28%] [G_gan loss: 2.176759, G_cl: 2.286075, recon: 0.390068] time: 0:10:19.853580 \n","[Epoch 0/170] [Batch 55/448] [D_gan loss: 0.474909, acc_gan:  74%] [D_cl loss: 0.102916, acc_cl:   2%] [G_gan loss: 2.153789, G_cl: 2.133135, recon: 0.373031] time: 0:11:19.460051 \n","[Epoch 0/170] [Batch 60/448] [D_gan loss: 0.302136, acc_gan:  89%] [D_cl loss: 0.087034, acc_cl:   6%] [G_gan loss: 2.256688, G_cl: 2.307898, recon: 0.379901] time: 0:12:18.178830 \n","[Epoch 0/170] [Batch 65/448] [D_gan loss: 0.297353, acc_gan:  87%] [D_cl loss: 0.085751, acc_cl:   8%] [G_gan loss: 2.866373, G_cl: 2.267049, recon: 0.380749] time: 0:13:16.276851 \n","[Epoch 0/170] [Batch 70/448] [D_gan loss: 0.263591, acc_gan:  89%] [D_cl loss: 0.080719, acc_cl:  30%] [G_gan loss: 2.603307, G_cl: 2.263905, recon: 0.375477] time: 0:14:16.313903 \n","[Epoch 0/170] [Batch 75/448] [D_gan loss: 0.314641, acc_gan:  91%] [D_cl loss: 0.085423, acc_cl:   3%] [G_gan loss: 2.014621, G_cl: 2.342680, recon: 0.378696] time: 0:15:16.784898 \n","[Epoch 0/170] [Batch 80/448] [D_gan loss: 0.219810, acc_gan:  92%] [D_cl loss: 0.078015, acc_cl:  12%] [G_gan loss: 2.985820, G_cl: 2.269200, recon: 0.372584] time: 0:16:17.443972 \n","[Epoch 0/170] [Batch 85/448] [D_gan loss: 0.268273, acc_gan:  95%] [D_cl loss: 0.085541, acc_cl:   4%] [G_gan loss: 3.707114, G_cl: 2.267983, recon: 0.385184] time: 0:17:16.665697 \n","[Epoch 0/170] [Batch 90/448] [D_gan loss: 0.252956, acc_gan:  92%] [D_cl loss: 0.080733, acc_cl:  15%] [G_gan loss: 2.921779, G_cl: 2.344351, recon: 0.378521] time: 0:18:14.635189 \n","[Epoch 0/170] [Batch 95/448] [D_gan loss: 0.237633, acc_gan:  91%] [D_cl loss: 0.082932, acc_cl:   4%] [G_gan loss: 2.280589, G_cl: 2.491642, recon: 0.370033] time: 0:19:21.482424 \n","[Epoch 0/170] [Batch 100/448] [D_gan loss: 0.189110, acc_gan:  92%] [D_cl loss: 0.077115, acc_cl:   5%] [G_gan loss: 2.439102, G_cl: 2.526538, recon: 0.358918] time: 0:20:22.160267 \n","[Epoch 0/170] [Batch 105/448] [D_gan loss: 0.360264, acc_gan:  88%] [D_cl loss: 0.091778, acc_cl:  17%] [G_gan loss: 1.976237, G_cl: 2.525674, recon: 0.373925] time: 0:21:22.393015 \n","[Epoch 0/170] [Batch 110/448] [D_gan loss: 0.243078, acc_gan:  91%] [D_cl loss: 0.092535, acc_cl:   9%] [G_gan loss: 2.523068, G_cl: 2.382298, recon: 0.363135] time: 0:22:20.573878 \n","[Epoch 0/170] [Batch 115/448] [D_gan loss: 0.230920, acc_gan:  93%] [D_cl loss: 0.079218, acc_cl:  11%] [G_gan loss: 3.307631, G_cl: 2.351904, recon: 0.374081] time: 0:23:20.462569 \n","[Epoch 0/170] [Batch 120/448] [D_gan loss: 0.344083, acc_gan:  89%] [D_cl loss: 0.087283, acc_cl:  12%] [G_gan loss: 3.980105, G_cl: 2.238549, recon: 0.363529] time: 0:24:16.424743 \n","[Epoch 0/170] [Batch 125/448] [D_gan loss: 0.152800, acc_gan:  93%] [D_cl loss: 0.085705, acc_cl:   9%] [G_gan loss: 3.811299, G_cl: 2.264823, recon: 0.366596] time: 0:25:13.721022 \n","[Epoch 0/170] [Batch 130/448] [D_gan loss: 0.191397, acc_gan:  95%] [D_cl loss: 0.079549, acc_cl:   6%] [G_gan loss: 4.847836, G_cl: 2.309749, recon: 0.362164] time: 0:26:13.017507 \n","[Epoch 0/170] [Batch 135/448] [D_gan loss: 0.066532, acc_gan:  98%] [D_cl loss: 0.066652, acc_cl:  30%] [G_gan loss: 4.003228, G_cl: 2.270432, recon: 0.354425] time: 0:27:13.515953 \n","[Epoch 0/170] [Batch 140/448] [D_gan loss: 0.268431, acc_gan:  92%] [D_cl loss: 0.078193, acc_cl:   4%] [G_gan loss: 3.114753, G_cl: 2.546057, recon: 0.345233] time: 0:28:12.888799 \n","[Epoch 0/170] [Batch 145/448] [D_gan loss: 0.070490, acc_gan:  98%] [D_cl loss: 0.065639, acc_cl:   6%] [G_gan loss: 3.685947, G_cl: 2.345263, recon: 0.355403] time: 0:29:11.260135 \n","[Epoch 0/170] [Batch 150/448] [D_gan loss: 0.087994, acc_gan:  99%] [D_cl loss: 0.064568, acc_cl:  13%] [G_gan loss: 4.853304, G_cl: 2.242318, recon: 0.346042] time: 0:30:11.637981 \n","[Epoch 0/170] [Batch 155/448] [D_gan loss: 0.129237, acc_gan:  96%] [D_cl loss: 0.069456, acc_cl:  11%] [G_gan loss: 4.014678, G_cl: 2.325633, recon: 0.344755] time: 0:31:08.811659 \n","[Epoch 0/170] [Batch 160/448] [D_gan loss: 0.628147, acc_gan:  75%] [D_cl loss: 0.103626, acc_cl:   5%] [G_gan loss: 3.416666, G_cl: 2.308124, recon: 0.330104] time: 0:32:05.901403 \n","[Epoch 0/170] [Batch 165/448] [D_gan loss: 0.310587, acc_gan:  87%] [D_cl loss: 0.090530, acc_cl:   2%] [G_gan loss: 2.470196, G_cl: 2.325520, recon: 0.331454] time: 0:33:04.278919 \n","[Epoch 0/170] [Batch 170/448] [D_gan loss: 0.255867, acc_gan:  88%] [D_cl loss: 0.084601, acc_cl:   5%] [G_gan loss: 2.360896, G_cl: 2.397632, recon: 0.329298] time: 0:34:02.292147 \n","[Epoch 0/170] [Batch 175/448] [D_gan loss: 0.184515, acc_gan:  93%] [D_cl loss: 0.084393, acc_cl:  19%] [G_gan loss: 3.055093, G_cl: 2.172356, recon: 0.324860] time: 0:34:59.695534 \n","[Epoch 0/170] [Batch 180/448] [D_gan loss: 0.228282, acc_gan:  91%] [D_cl loss: 0.081642, acc_cl:  36%] [G_gan loss: 3.722525, G_cl: 2.217926, recon: 0.318208] time: 0:35:58.956194 \n","[Epoch 0/170] [Batch 185/448] [D_gan loss: 0.143229, acc_gan:  95%] [D_cl loss: 0.073881, acc_cl:   3%] [G_gan loss: 2.952631, G_cl: 2.300509, recon: 0.335533] time: 0:36:58.625718 \n","[Epoch 0/170] [Batch 190/448] [D_gan loss: 0.177720, acc_gan:  93%] [D_cl loss: 0.078000, acc_cl:   8%] [G_gan loss: 2.111961, G_cl: 2.267451, recon: 0.307599] time: 0:37:57.807685 \n","[Epoch 0/170] [Batch 195/448] [D_gan loss: 0.124373, acc_gan:  96%] [D_cl loss: 0.071518, acc_cl:  18%] [G_gan loss: 3.059641, G_cl: 2.248838, recon: 0.312169] time: 0:38:56.989654 \n","[Epoch 0/170] [Batch 200/448] [D_gan loss: 0.155954, acc_gan:  95%] [D_cl loss: 0.073732, acc_cl:   3%] [G_gan loss: 3.494271, G_cl: 2.239069, recon: 0.316385] time: 0:39:55.212934 \n","*** TEST *** [D_gan accuracy : 0.75 ] [D_cl accuracy : 0.375 ]\n","[Epoch 0/170] [Batch 205/448] [D_gan loss: 0.063479, acc_gan:  98%] [D_cl loss: 0.060504, acc_cl:   6%] [G_gan loss: 3.596260, G_cl: 2.304622, recon: 0.304869] time: 0:41:01.272000 \n","[Epoch 0/170] [Batch 210/448] [D_gan loss: 0.194478, acc_gan:  93%] [D_cl loss: 0.079172, acc_cl:   4%] [G_gan loss: 3.301696, G_cl: 2.224456, recon: 0.285205] time: 0:42:01.877918 \n","[Epoch 0/170] [Batch 215/448] [D_gan loss: 0.140742, acc_gan:  94%] [D_cl loss: 0.067695, acc_cl:   6%] [G_gan loss: 3.902483, G_cl: 2.138446, recon: 0.284835] time: 0:42:57.335968 \n","[Epoch 0/170] [Batch 220/448] [D_gan loss: 0.095653, acc_gan:  97%] [D_cl loss: 0.066656, acc_cl:   5%] [G_gan loss: 3.146908, G_cl: 2.307693, recon: 0.283352] time: 0:43:55.617707 \n","[Epoch 0/170] [Batch 225/448] [D_gan loss: 0.227552, acc_gan:  91%] [D_cl loss: 0.072149, acc_cl:   5%] [G_gan loss: 2.432068, G_cl: 2.362373, recon: 0.261606] time: 0:44:53.260650 \n","[Epoch 0/170] [Batch 230/448] [D_gan loss: 0.158114, acc_gan:  95%] [D_cl loss: 0.070402, acc_cl:   5%] [G_gan loss: 3.425845, G_cl: 2.285032, recon: 0.253812] time: 0:45:51.845684 \n","[Epoch 0/170] [Batch 235/448] [D_gan loss: 0.115919, acc_gan:  97%] [D_cl loss: 0.069073, acc_cl:   5%] [G_gan loss: 3.855364, G_cl: 2.237246, recon: 0.260486] time: 0:46:50.120900 \n","[Epoch 0/170] [Batch 240/448] [D_gan loss: 0.307950, acc_gan:  89%] [D_cl loss: 0.087792, acc_cl:   4%] [G_gan loss: 2.082862, G_cl: 2.357315, recon: 0.244026] time: 0:47:46.444425 \n","[Epoch 0/170] [Batch 245/448] [D_gan loss: 0.222980, acc_gan:  92%] [D_cl loss: 0.076835, acc_cl:   3%] [G_gan loss: 2.725413, G_cl: 2.291966, recon: 0.248571] time: 0:48:45.654769 \n","[Epoch 0/170] [Batch 250/448] [D_gan loss: 0.224838, acc_gan:  92%] [D_cl loss: 0.077144, acc_cl:   8%] [G_gan loss: 2.629704, G_cl: 2.345907, recon: 0.231387] time: 0:49:44.860462 \n","[Epoch 0/170] [Batch 255/448] [D_gan loss: 0.184402, acc_gan:  94%] [D_cl loss: 0.072059, acc_cl:   7%] [G_gan loss: 3.001270, G_cl: 2.339679, recon: 0.233712] time: 0:50:42.329573 \n","[Epoch 0/170] [Batch 260/448] [D_gan loss: 0.286508, acc_gan:  90%] [D_cl loss: 0.085514, acc_cl:   6%] [G_gan loss: 2.710053, G_cl: 2.300450, recon: 0.225358] time: 0:51:39.356228 \n","[Epoch 0/170] [Batch 265/448] [D_gan loss: 0.193148, acc_gan:  93%] [D_cl loss: 0.077688, acc_cl:   5%] [G_gan loss: 2.972728, G_cl: 2.233895, recon: 0.231211] time: 0:52:36.330725 \n","[Epoch 0/170] [Batch 270/448] [D_gan loss: 0.175977, acc_gan:  92%] [D_cl loss: 0.073176, acc_cl:  10%] [G_gan loss: 3.107188, G_cl: 2.266382, recon: 0.238649] time: 0:53:33.310833 \n","[Epoch 0/170] [Batch 275/448] [D_gan loss: 0.127854, acc_gan:  95%] [D_cl loss: 0.067047, acc_cl:   8%] [G_gan loss: 3.302792, G_cl: 2.280210, recon: 0.222901] time: 0:54:31.366506 \n","[Epoch 0/170] [Batch 280/448] [D_gan loss: 0.277612, acc_gan:  91%] [D_cl loss: 0.086095, acc_cl:   2%] [G_gan loss: 2.422002, G_cl: 2.394380, recon: 0.220072] time: 0:55:28.376841 \n","[Epoch 0/170] [Batch 285/448] [D_gan loss: 0.246392, acc_gan:  91%] [D_cl loss: 0.078391, acc_cl:   7%] [G_gan loss: 2.665303, G_cl: 2.427457, recon: 0.229630] time: 0:56:25.391957 \n","[Epoch 0/170] [Batch 290/448] [D_gan loss: 0.254188, acc_gan:  90%] [D_cl loss: 0.082074, acc_cl:   9%] [G_gan loss: 2.517993, G_cl: 2.498959, recon: 0.223508] time: 0:57:25.446323 \n","[Epoch 0/170] [Batch 295/448] [D_gan loss: 0.126664, acc_gan:  95%] [D_cl loss: 0.072694, acc_cl:   7%] [G_gan loss: 3.132148, G_cl: 2.308989, recon: 0.233094] time: 0:58:24.536867 \n","[Epoch 0/170] [Batch 300/448] [D_gan loss: 0.218111, acc_gan:  91%] [D_cl loss: 0.081027, acc_cl:  58%] [G_gan loss: 2.611787, G_cl: 2.409601, recon: 0.214458] time: 0:59:22.314027 \n","[Epoch 0/170] [Batch 305/448] [D_gan loss: 0.127540, acc_gan:  95%] [D_cl loss: 0.070771, acc_cl:   8%] [G_gan loss: 3.942338, G_cl: 2.282170, recon: 0.204721] time: 1:00:22.493938 \n","[Epoch 0/170] [Batch 310/448] [D_gan loss: 0.156999, acc_gan:  95%] [D_cl loss: 0.069600, acc_cl:  38%] [G_gan loss: 2.833638, G_cl: 2.485618, recon: 0.212956] time: 1:01:20.183239 \n","[Epoch 0/170] [Batch 315/448] [D_gan loss: 0.276269, acc_gan:  90%] [D_cl loss: 0.080740, acc_cl:  28%] [G_gan loss: 2.543962, G_cl: 2.483644, recon: 0.233732] time: 1:02:17.133855 \n","[Epoch 0/170] [Batch 320/448] [D_gan loss: 0.135824, acc_gan:  95%] [D_cl loss: 0.065204, acc_cl:   5%] [G_gan loss: 3.411432, G_cl: 2.382434, recon: 0.244195] time: 1:03:15.353737 \n","[Epoch 0/170] [Batch 325/448] [D_gan loss: 0.066040, acc_gan:  98%] [D_cl loss: 0.065653, acc_cl:   9%] [G_gan loss: 3.735994, G_cl: 2.324078, recon: 0.246416] time: 1:04:13.062820 \n","[Epoch 0/170] [Batch 330/448] [D_gan loss: 0.264462, acc_gan:  91%] [D_cl loss: 0.083182, acc_cl:   4%] [G_gan loss: 2.553226, G_cl: 2.616537, recon: 0.234052] time: 1:05:09.991842 \n","[Epoch 0/170] [Batch 335/448] [D_gan loss: 0.208963, acc_gan:  91%] [D_cl loss: 0.076954, acc_cl:   7%] [G_gan loss: 3.203460, G_cl: 2.336570, recon: 0.244425] time: 1:06:05.236505 \n","[Epoch 0/170] [Batch 340/448] [D_gan loss: 0.106730, acc_gan:  96%] [D_cl loss: 0.065576, acc_cl:   9%] [G_gan loss: 3.323205, G_cl: 2.366198, recon: 0.258303] time: 1:07:01.860051 \n","[Epoch 0/170] [Batch 345/448] [D_gan loss: 0.105558, acc_gan:  96%] [D_cl loss: 0.061703, acc_cl:   9%] [G_gan loss: 3.421342, G_cl: 2.386091, recon: 0.248500] time: 1:08:00.624913 \n","[Epoch 0/170] [Batch 350/448] [D_gan loss: 0.301529, acc_gan:  89%] [D_cl loss: 0.083735, acc_cl:  10%] [G_gan loss: 2.593446, G_cl: 2.562172, recon: 0.209423] time: 1:08:57.250030 \n","[Epoch 0/170] [Batch 355/448] [D_gan loss: 0.240504, acc_gan:  91%] [D_cl loss: 0.079029, acc_cl:   8%] [G_gan loss: 2.746919, G_cl: 2.359119, recon: 0.205053] time: 1:09:55.954140 \n","[Epoch 0/170] [Batch 360/448] [D_gan loss: 0.294419, acc_gan:  89%] [D_cl loss: 0.081035, acc_cl:  12%] [G_gan loss: 2.193493, G_cl: 2.546084, recon: 0.205165] time: 1:10:53.722887 \n","[Epoch 0/170] [Batch 365/448] [D_gan loss: 0.195182, acc_gan:  93%] [D_cl loss: 0.077697, acc_cl:  14%] [G_gan loss: 2.920789, G_cl: 2.352879, recon: 0.206453] time: 1:11:52.764054 \n","[Epoch 0/170] [Batch 370/448] [D_gan loss: 0.195321, acc_gan:  91%] [D_cl loss: 0.073701, acc_cl:  35%] [G_gan loss: 2.786903, G_cl: 2.410542, recon: 0.185685] time: 1:12:49.548859 \n","[Epoch 0/170] [Batch 375/448] [D_gan loss: 0.231797, acc_gan:  92%] [D_cl loss: 0.073562, acc_cl:   5%] [G_gan loss: 2.710687, G_cl: 2.565796, recon: 0.207255] time: 1:13:47.434445 \n","[Epoch 0/170] [Batch 380/448] [D_gan loss: 0.129338, acc_gan:  95%] [D_cl loss: 0.065544, acc_cl:   6%] [G_gan loss: 3.209653, G_cl: 2.430009, recon: 0.210492] time: 1:14:44.070005 \n","[Epoch 0/170] [Batch 385/448] [D_gan loss: 0.228838, acc_gan:  91%] [D_cl loss: 0.077649, acc_cl:   7%] [G_gan loss: 2.448663, G_cl: 2.509151, recon: 0.198764] time: 1:15:41.913498 \n","[Epoch 0/170] [Batch 390/448] [D_gan loss: 0.195957, acc_gan:  93%] [D_cl loss: 0.076937, acc_cl:   8%] [G_gan loss: 3.081088, G_cl: 2.420178, recon: 0.191514] time: 1:16:38.728656 \n","[Epoch 0/170] [Batch 395/448] [D_gan loss: 0.302593, acc_gan:  87%] [D_cl loss: 0.089544, acc_cl:   5%] [G_gan loss: 2.213013, G_cl: 2.490191, recon: 0.192863] time: 1:17:36.878228 \n","[Epoch 0/170] [Batch 400/448] [D_gan loss: 0.202448, acc_gan:  92%] [D_cl loss: 0.072621, acc_cl:   9%] [G_gan loss: 2.888108, G_cl: 2.449616, recon: 0.201670] time: 1:18:34.714841 \n","*** TEST *** [D_gan accuracy : 0.5 ] [D_cl accuracy : 0.390625 ]\n","[Epoch 0/170] [Batch 405/448] [D_gan loss: 0.193362, acc_gan:  94%] [D_cl loss: 0.075749, acc_cl:  25%] [G_gan loss: 2.996212, G_cl: 2.385960, recon: 0.191285] time: 1:19:30.134046 \n","[Epoch 0/170] [Batch 410/448] [D_gan loss: 0.136801, acc_gan:  95%] [D_cl loss: 0.071282, acc_cl:  11%] [G_gan loss: 3.369815, G_cl: 2.258512, recon: 0.215957] time: 1:20:26.800619 \n","[Epoch 0/170] [Batch 415/448] [D_gan loss: 0.103251, acc_gan:  96%] [D_cl loss: 0.061659, acc_cl:  21%] [G_gan loss: 3.824618, G_cl: 2.355396, recon: 0.199759] time: 1:21:24.786206 \n","[Epoch 0/170] [Batch 420/448] [D_gan loss: 0.212328, acc_gan:  92%] [D_cl loss: 0.073787, acc_cl:  32%] [G_gan loss: 2.659921, G_cl: 2.400593, recon: 0.213693] time: 1:22:22.417814 \n","[Epoch 0/170] [Batch 425/448] [D_gan loss: 0.143860, acc_gan:  95%] [D_cl loss: 0.070465, acc_cl:  15%] [G_gan loss: 3.317797, G_cl: 2.404467, recon: 0.227407] time: 1:23:20.146172 \n","[Epoch 0/170] [Batch 430/448] [D_gan loss: 0.176259, acc_gan:  92%] [D_cl loss: 0.070609, acc_cl:  12%] [G_gan loss: 3.391736, G_cl: 2.359666, recon: 0.218352] time: 1:24:15.693994 \n","[Epoch 0/170] [Batch 435/448] [D_gan loss: 0.159175, acc_gan:  95%] [D_cl loss: 0.071872, acc_cl:  15%] [G_gan loss: 3.373527, G_cl: 2.432561, recon: 0.209537] time: 1:25:13.347113 \n","[Epoch 0/170] [Batch 440/448] [D_gan loss: 0.164245, acc_gan:  94%] [D_cl loss: 0.067339, acc_cl:  23%] [G_gan loss: 2.869671, G_cl: 2.435519, recon: 0.196519] time: 1:26:12.273233 \n","[Epoch 0/170] [Batch 445/448] [D_gan loss: 0.275818, acc_gan:  90%] [D_cl loss: 0.082072, acc_cl:  14%] [G_gan loss: 2.806764, G_cl: 2.440792, recon: 0.184613] time: 1:27:11.338222 \n","[Epoch 1/170] [Batch 0/448] [D_gan loss: 0.234242, acc_gan:  91%] [D_cl loss: 0.073352, acc_cl:  25%] [G_gan loss: 2.772605, G_cl: 2.519563, recon: 0.198729] time: 1:27:53.226529 \n","*** TEST *** [D_gan accuracy : 0.453125 ] [D_cl accuracy : 0.453125 ]\n","[Epoch 1/170] [Batch 5/448] [D_gan loss: 0.112413, acc_gan:  96%] [D_cl loss: 0.065130, acc_cl:  14%] [G_gan loss: 3.549747, G_cl: 2.415712, recon: 0.193436] time: 1:28:49.544424 \n","[Epoch 1/170] [Batch 10/448] [D_gan loss: 0.103345, acc_gan:  96%] [D_cl loss: 0.067417, acc_cl:  14%] [G_gan loss: 3.669900, G_cl: 2.326895, recon: 0.203527] time: 1:29:46.212462 \n","[Epoch 1/170] [Batch 15/448] [D_gan loss: 0.045673, acc_gan:  98%] [D_cl loss: 0.059603, acc_cl:   6%] [G_gan loss: 4.994245, G_cl: 2.231511, recon: 0.233666] time: 1:30:43.841322 \n","[Epoch 1/170] [Batch 20/448] [D_gan loss: 0.221126, acc_gan:  92%] [D_cl loss: 0.092368, acc_cl:   4%] [G_gan loss: 2.785476, G_cl: 2.377238, recon: 0.245765] time: 1:31:41.779310 \n","[Epoch 1/170] [Batch 25/448] [D_gan loss: 0.074111, acc_gan:  96%] [D_cl loss: 0.067068, acc_cl:   4%] [G_gan loss: 4.358122, G_cl: 2.398442, recon: 0.265942] time: 1:32:38.345149 \n","[Epoch 1/170] [Batch 30/448] [D_gan loss: 0.028356, acc_gan:  99%] [D_cl loss: 0.058986, acc_cl:   4%] [G_gan loss: 4.922092, G_cl: 2.291936, recon: 0.264346] time: 1:33:34.825202 \n","[Epoch 1/170] [Batch 35/448] [D_gan loss: 0.351842, acc_gan:  88%] [D_cl loss: 0.085278, acc_cl:  18%] [G_gan loss: 2.503817, G_cl: 2.613649, recon: 0.274579] time: 1:34:30.497166 \n","[Epoch 1/170] [Batch 40/448] [D_gan loss: 0.240364, acc_gan:  90%] [D_cl loss: 0.074176, acc_cl:   8%] [G_gan loss: 2.841254, G_cl: 2.502521, recon: 0.282874] time: 1:35:28.217398 \n","[Epoch 1/170] [Batch 45/448] [D_gan loss: 0.172586, acc_gan:  93%] [D_cl loss: 0.072059, acc_cl:   5%] [G_gan loss: 3.120833, G_cl: 2.446207, recon: 0.269552] time: 1:36:23.682646 \n","[Epoch 1/170] [Batch 50/448] [D_gan loss: 0.094861, acc_gan:  96%] [D_cl loss: 0.066443, acc_cl:  34%] [G_gan loss: 3.950860, G_cl: 2.320917, recon: 0.276746] time: 1:37:20.525818 \n","[Epoch 1/170] [Batch 55/448] [D_gan loss: 0.198945, acc_gan:  93%] [D_cl loss: 0.075362, acc_cl:  16%] [G_gan loss: 2.316669, G_cl: 2.603396, recon: 0.267461] time: 1:38:17.122385 \n","[Epoch 1/170] [Batch 60/448] [D_gan loss: 0.137191, acc_gan:  95%] [D_cl loss: 0.066318, acc_cl:   9%] [G_gan loss: 3.645237, G_cl: 2.497277, recon: 0.273184] time: 1:39:14.822894 \n","[Epoch 1/170] [Batch 65/448] [D_gan loss: 0.105113, acc_gan:  96%] [D_cl loss: 0.062120, acc_cl:  45%] [G_gan loss: 3.845774, G_cl: 2.452929, recon: 0.266224] time: 1:40:12.740264 \n","[Epoch 1/170] [Batch 70/448] [D_gan loss: 0.118381, acc_gan:  95%] [D_cl loss: 0.060137, acc_cl:  24%] [G_gan loss: 3.408170, G_cl: 2.538125, recon: 0.250030] time: 1:41:10.237750 \n","[Epoch 1/170] [Batch 75/448] [D_gan loss: 0.273263, acc_gan:  91%] [D_cl loss: 0.079235, acc_cl:  12%] [G_gan loss: 2.846835, G_cl: 2.379617, recon: 0.237397] time: 1:42:07.875787 \n","[Epoch 1/170] [Batch 80/448] [D_gan loss: 0.182479, acc_gan:  91%] [D_cl loss: 0.072475, acc_cl:  17%] [G_gan loss: 3.194181, G_cl: 2.347863, recon: 0.218286] time: 1:43:04.569528 \n","[Epoch 1/170] [Batch 85/448] [D_gan loss: 0.168227, acc_gan:  92%] [D_cl loss: 0.064721, acc_cl:  12%] [G_gan loss: 3.466532, G_cl: 2.508523, recon: 0.220473] time: 1:44:01.247629 \n","[Epoch 1/170] [Batch 90/448] [D_gan loss: 0.131702, acc_gan:  96%] [D_cl loss: 0.065740, acc_cl:  11%] [G_gan loss: 3.303571, G_cl: 2.658681, recon: 0.223862] time: 1:44:56.686062 \n","[Epoch 1/170] [Batch 95/448] [D_gan loss: 0.124040, acc_gan:  95%] [D_cl loss: 0.065589, acc_cl:  11%] [G_gan loss: 4.121282, G_cl: 2.432504, recon: 0.205068] time: 1:45:53.279093 \n","[Epoch 1/170] [Batch 100/448] [D_gan loss: 0.173582, acc_gan:  95%] [D_cl loss: 0.071011, acc_cl:  20%] [G_gan loss: 3.276076, G_cl: 2.484333, recon: 0.194712] time: 1:46:50.072005 \n","[Epoch 1/170] [Batch 105/448] [D_gan loss: 0.140034, acc_gan:  94%] [D_cl loss: 0.074483, acc_cl:  12%] [G_gan loss: 3.785894, G_cl: 2.392821, recon: 0.196660] time: 1:47:46.647112 \n","[Epoch 1/170] [Batch 110/448] [D_gan loss: 0.136493, acc_gan:  95%] [D_cl loss: 0.070218, acc_cl:  19%] [G_gan loss: 4.317847, G_cl: 2.311878, recon: 0.194645] time: 1:48:43.242408 \n","[Epoch 1/170] [Batch 115/448] [D_gan loss: 0.094630, acc_gan:  96%] [D_cl loss: 0.069895, acc_cl:   5%] [G_gan loss: 6.252514, G_cl: 2.319347, recon: 0.196203] time: 1:49:39.655231 \n","[Epoch 1/170] [Batch 120/448] [D_gan loss: 0.031100, acc_gan:  98%] [D_cl loss: 0.063083, acc_cl:   5%] [G_gan loss: 5.735891, G_cl: 2.267732, recon: 0.216975] time: 1:50:36.304490 \n","[Epoch 1/170] [Batch 125/448] [D_gan loss: 0.032666, acc_gan:  98%] [D_cl loss: 0.057914, acc_cl:   8%] [G_gan loss: 5.574661, G_cl: 2.303900, recon: 0.231371] time: 1:51:32.696664 \n","[Epoch 1/170] [Batch 130/448] [D_gan loss: 0.043053, acc_gan:  98%] [D_cl loss: 0.061114, acc_cl:  10%] [G_gan loss: 4.531732, G_cl: 2.350964, recon: 0.254618] time: 1:52:28.068351 \n","[Epoch 1/170] [Batch 135/448] [D_gan loss: 0.032842, acc_gan:  98%] [D_cl loss: 0.059024, acc_cl:  18%] [G_gan loss: 6.176626, G_cl: 2.318054, recon: 0.272739] time: 1:53:23.653755 \n","[Epoch 1/170] [Batch 140/448] [D_gan loss: 0.132380, acc_gan:  95%] [D_cl loss: 0.063676, acc_cl:   6%] [G_gan loss: 3.224542, G_cl: 2.481594, recon: 0.267577] time: 1:54:20.361608 \n","[Epoch 1/170] [Batch 145/448] [D_gan loss: 0.073127, acc_gan:  97%] [D_cl loss: 0.058069, acc_cl:   8%] [G_gan loss: 4.955761, G_cl: 2.386165, recon: 0.286220] time: 1:55:16.052196 \n","[Epoch 1/170] [Batch 150/448] [D_gan loss: 0.039653, acc_gan:  99%] [D_cl loss: 0.055166, acc_cl:   7%] [G_gan loss: 4.532335, G_cl: 2.450347, recon: 0.276170] time: 1:56:13.861928 \n","[Epoch 1/170] [Batch 155/448] [D_gan loss: 0.075707, acc_gan:  97%] [D_cl loss: 0.056984, acc_cl:   5%] [G_gan loss: 4.729730, G_cl: 2.422092, recon: 0.283219] time: 1:57:09.580178 \n","[Epoch 1/170] [Batch 160/448] [D_gan loss: 0.052121, acc_gan:  97%] [D_cl loss: 0.057429, acc_cl:  13%] [G_gan loss: 5.233025, G_cl: 2.471555, recon: 0.274003] time: 1:58:07.202828 \n","[Epoch 1/170] [Batch 165/448] [D_gan loss: 0.019460, acc_gan:  99%] [D_cl loss: 0.057191, acc_cl:   5%] [G_gan loss: 5.521803, G_cl: 2.357755, recon: 0.258471] time: 1:59:06.073302 \n","[Epoch 1/170] [Batch 170/448] [D_gan loss: 0.019524, acc_gan:  99%] [D_cl loss: 0.052858, acc_cl:   5%] [G_gan loss: 5.567233, G_cl: 2.340916, recon: 0.253718] time: 2:00:04.338025 \n","[Epoch 1/170] [Batch 175/448] [D_gan loss: 0.366135, acc_gan:  88%] [D_cl loss: 0.095235, acc_cl:  15%] [G_gan loss: 2.104111, G_cl: 2.551001, recon: 0.222181] time: 2:01:03.468944 \n","[Epoch 1/170] [Batch 180/448] [D_gan loss: 0.213913, acc_gan:  94%] [D_cl loss: 0.082367, acc_cl:   4%] [G_gan loss: 3.373739, G_cl: 2.526228, recon: 0.239050] time: 2:02:01.367711 \n","[Epoch 1/170] [Batch 185/448] [D_gan loss: 0.183045, acc_gan:  94%] [D_cl loss: 0.075579, acc_cl:   5%] [G_gan loss: 3.087889, G_cl: 2.608883, recon: 0.230761] time: 2:02:59.114058 \n","[Epoch 1/170] [Batch 190/448] [D_gan loss: 0.198164, acc_gan:  93%] [D_cl loss: 0.075597, acc_cl:   9%] [G_gan loss: 3.247975, G_cl: 2.350008, recon: 0.215305] time: 2:03:58.239043 \n","[Epoch 1/170] [Batch 195/448] [D_gan loss: 0.213635, acc_gan:  93%] [D_cl loss: 0.071099, acc_cl:  18%] [G_gan loss: 3.486068, G_cl: 2.461152, recon: 0.222201] time: 2:04:54.934730 \n","[Epoch 1/170] [Batch 200/448] [D_gan loss: 0.179840, acc_gan:  94%] [D_cl loss: 0.074212, acc_cl:   8%] [G_gan loss: 3.001936, G_cl: 2.419151, recon: 0.201328] time: 2:05:55.048855 \n","*** TEST *** [D_gan accuracy : 0.671875 ] [D_cl accuracy : 0.265625 ]\n","[Epoch 1/170] [Batch 205/448] [D_gan loss: 0.206459, acc_gan:  93%] [D_cl loss: 0.073530, acc_cl:   9%] [G_gan loss: 3.129613, G_cl: 2.559975, recon: 0.199378] time: 2:06:53.663118 \n","[Epoch 1/170] [Batch 210/448] [D_gan loss: 0.229199, acc_gan:  89%] [D_cl loss: 0.091267, acc_cl:   3%] [G_gan loss: 3.093580, G_cl: 2.558795, recon: 0.199538] time: 2:07:49.456337 \n","[Epoch 1/170] [Batch 215/448] [D_gan loss: 0.143738, acc_gan:  95%] [D_cl loss: 0.073456, acc_cl:  29%] [G_gan loss: 3.151561, G_cl: 2.547207, recon: 0.194473] time: 2:08:47.560848 \n","[Epoch 1/170] [Batch 220/448] [D_gan loss: 0.073902, acc_gan:  98%] [D_cl loss: 0.063554, acc_cl:   4%] [G_gan loss: 4.424151, G_cl: 2.367961, recon: 0.201767] time: 2:09:43.140714 \n","[Epoch 1/170] [Batch 225/448] [D_gan loss: 0.037128, acc_gan:  99%] [D_cl loss: 0.056172, acc_cl:  12%] [G_gan loss: 5.721939, G_cl: 2.194613, recon: 0.197172] time: 2:10:41.050032 \n","[Epoch 1/170] [Batch 230/448] [D_gan loss: 0.327470, acc_gan:  94%] [D_cl loss: 0.076233, acc_cl:   5%] [G_gan loss: 4.484569, G_cl: 2.412985, recon: 0.209776] time: 2:11:38.653226 \n","[Epoch 1/170] [Batch 235/448] [D_gan loss: 0.184862, acc_gan:  95%] [D_cl loss: 0.069624, acc_cl:   6%] [G_gan loss: 3.812499, G_cl: 2.443918, recon: 0.207056] time: 2:12:34.194736 \n","[Epoch 1/170] [Batch 240/448] [D_gan loss: 0.148110, acc_gan:  95%] [D_cl loss: 0.069556, acc_cl:   6%] [G_gan loss: 4.049758, G_cl: 2.346469, recon: 0.198045] time: 2:13:31.997062 \n","[Epoch 1/170] [Batch 245/448] [D_gan loss: 0.148457, acc_gan:  93%] [D_cl loss: 0.064776, acc_cl:   6%] [G_gan loss: 3.635897, G_cl: 2.469186, recon: 0.199961] time: 2:14:27.549576 \n","[Epoch 1/170] [Batch 250/448] [D_gan loss: 0.166693, acc_gan:  93%] [D_cl loss: 0.066075, acc_cl:  14%] [G_gan loss: 3.909508, G_cl: 2.393804, recon: 0.205401] time: 2:15:23.122486 \n","[Epoch 1/170] [Batch 255/448] [D_gan loss: 0.109301, acc_gan:  95%] [D_cl loss: 0.064557, acc_cl:  13%] [G_gan loss: 4.462922, G_cl: 2.366999, recon: 0.198679] time: 2:16:20.952386 \n","[Epoch 1/170] [Batch 260/448] [D_gan loss: 0.091760, acc_gan:  95%] [D_cl loss: 0.057287, acc_cl:  12%] [G_gan loss: 4.869831, G_cl: 2.398552, recon: 0.199879] time: 2:17:18.776085 \n","[Epoch 1/170] [Batch 265/448] [D_gan loss: 0.082770, acc_gan:  97%] [D_cl loss: 0.060000, acc_cl:   8%] [G_gan loss: 4.427471, G_cl: 2.406635, recon: 0.206421] time: 2:18:15.426979 \n","[Epoch 1/170] [Batch 270/448] [D_gan loss: 0.027113, acc_gan:  98%] [D_cl loss: 0.051642, acc_cl:  14%] [G_gan loss: 5.844227, G_cl: 2.138968, recon: 0.209673] time: 2:19:13.161411 \n","[Epoch 1/170] [Batch 275/448] [D_gan loss: 0.029515, acc_gan:  98%] [D_cl loss: 0.056393, acc_cl:  27%] [G_gan loss: 6.806628, G_cl: 2.098959, recon: 0.204260] time: 2:20:09.981207 \n","[Epoch 1/170] [Batch 280/448] [D_gan loss: 0.035800, acc_gan:  98%] [D_cl loss: 0.053863, acc_cl:  37%] [G_gan loss: 5.906395, G_cl: 2.245948, recon: 0.204601] time: 2:21:06.542230 \n","[Epoch 1/170] [Batch 285/448] [D_gan loss: 0.048242, acc_gan:  98%] [D_cl loss: 0.054502, acc_cl:  22%] [G_gan loss: 5.849878, G_cl: 2.280622, recon: 0.210750] time: 2:22:04.237137 \n","[Epoch 1/170] [Batch 290/448] [D_gan loss: 0.045341, acc_gan:  98%] [D_cl loss: 0.054384, acc_cl:  18%] [G_gan loss: 5.138846, G_cl: 2.355351, recon: 0.220957] time: 2:22:59.973788 \n","[Epoch 1/170] [Batch 295/448] [D_gan loss: 0.037708, acc_gan:  97%] [D_cl loss: 0.057711, acc_cl:  17%] [G_gan loss: 6.637340, G_cl: 2.318464, recon: 0.233881] time: 2:23:59.043773 \n","[Epoch 1/170] [Batch 300/448] [D_gan loss: 0.011454, acc_gan: 100%] [D_cl loss: 0.060513, acc_cl:  18%] [G_gan loss: 6.217233, G_cl: 2.233001, recon: 0.229440] time: 2:24:58.046441 \n","[Epoch 1/170] [Batch 305/448] [D_gan loss: 0.048036, acc_gan:  98%] [D_cl loss: 0.056573, acc_cl:  20%] [G_gan loss: 5.265938, G_cl: 2.511890, recon: 0.225869] time: 2:25:55.986231 \n","[Epoch 1/170] [Batch 310/448] [D_gan loss: 0.458945, acc_gan:  93%] [D_cl loss: 0.086978, acc_cl:   3%] [G_gan loss: 5.348036, G_cl: 2.496692, recon: 0.261533] time: 2:26:53.813344 \n","[Epoch 1/170] [Batch 315/448] [D_gan loss: 0.137936, acc_gan:  97%] [D_cl loss: 0.066163, acc_cl:  55%] [G_gan loss: 3.115101, G_cl: 2.377682, recon: 0.244552] time: 2:27:49.444680 \n","[Epoch 1/170] [Batch 320/448] [D_gan loss: 0.061784, acc_gan:  97%] [D_cl loss: 0.060534, acc_cl:   5%] [G_gan loss: 4.478784, G_cl: 2.315309, recon: 0.248851] time: 2:28:46.135981 \n","[Epoch 1/170] [Batch 325/448] [D_gan loss: 0.016965, acc_gan:  99%] [D_cl loss: 0.055322, acc_cl:   6%] [G_gan loss: 5.445590, G_cl: 2.251710, recon: 0.248694] time: 2:29:45.086931 \n","[Epoch 1/170] [Batch 330/448] [D_gan loss: 0.132576, acc_gan:  95%] [D_cl loss: 0.069483, acc_cl:  11%] [G_gan loss: 4.809991, G_cl: 2.420591, recon: 0.248323] time: 2:30:41.767170 \n","[Epoch 1/170] [Batch 335/448] [D_gan loss: 0.051485, acc_gan:  97%] [D_cl loss: 0.057438, acc_cl:  13%] [G_gan loss: 6.375486, G_cl: 2.216651, recon: 0.238655] time: 2:31:37.293248 \n","[Epoch 1/170] [Batch 340/448] [D_gan loss: 0.101424, acc_gan:  96%] [D_cl loss: 0.058800, acc_cl:  32%] [G_gan loss: 4.703900, G_cl: 2.456473, recon: 0.230893] time: 2:32:37.541080 \n","[Epoch 1/170] [Batch 345/448] [D_gan loss: 0.038186, acc_gan:  99%] [D_cl loss: 0.054835, acc_cl:  12%] [G_gan loss: 5.249721, G_cl: 2.283633, recon: 0.250151] time: 2:33:35.307399 \n","[Epoch 1/170] [Batch 350/448] [D_gan loss: 0.042613, acc_gan:  98%] [D_cl loss: 0.056696, acc_cl:  39%] [G_gan loss: 5.968502, G_cl: 2.262337, recon: 0.227172] time: 2:34:34.304620 \n","[Epoch 1/170] [Batch 355/448] [D_gan loss: 0.054744, acc_gan:  98%] [D_cl loss: 0.057174, acc_cl:  37%] [G_gan loss: 5.729301, G_cl: 2.312600, recon: 0.231270] time: 2:35:33.028472 \n","[Epoch 1/170] [Batch 360/448] [D_gan loss: 0.055104, acc_gan:  97%] [D_cl loss: 0.058942, acc_cl:  35%] [G_gan loss: 5.386686, G_cl: 2.359967, recon: 0.240835] time: 2:36:28.458818 \n","[Epoch 1/170] [Batch 365/448] [D_gan loss: 0.035925, acc_gan:  98%] [D_cl loss: 0.050097, acc_cl:  11%] [G_gan loss: 6.342144, G_cl: 2.301790, recon: 0.227262] time: 2:37:25.176696 \n","[Epoch 1/170] [Batch 370/448] [D_gan loss: 0.021866, acc_gan:  99%] [D_cl loss: 0.055937, acc_cl:  19%] [G_gan loss: 7.223128, G_cl: 2.145859, recon: 0.217873] time: 2:38:24.178414 \n","[Epoch 1/170] [Batch 375/448] [D_gan loss: 0.035772, acc_gan:  98%] [D_cl loss: 0.052376, acc_cl:  24%] [G_gan loss: 7.152195, G_cl: 2.216186, recon: 0.220141] time: 2:39:23.150966 \n","[Epoch 1/170] [Batch 380/448] [D_gan loss: 0.029382, acc_gan:  99%] [D_cl loss: 0.057068, acc_cl:  17%] [G_gan loss: 5.852917, G_cl: 2.360860, recon: 0.230404] time: 2:40:19.826628 \n","[Epoch 1/170] [Batch 385/448] [D_gan loss: 0.133251, acc_gan:  97%] [D_cl loss: 0.068805, acc_cl:  35%] [G_gan loss: 6.436224, G_cl: 2.282233, recon: 0.234314] time: 2:41:17.587509 \n","[Epoch 1/170] [Batch 390/448] [D_gan loss: 0.068734, acc_gan:  98%] [D_cl loss: 0.054292, acc_cl:  26%] [G_gan loss: 5.846031, G_cl: 2.356432, recon: 0.227789] time: 2:42:15.384092 \n","[Epoch 1/170] [Batch 395/448] [D_gan loss: 0.030629, acc_gan:  99%] [D_cl loss: 0.058627, acc_cl:  29%] [G_gan loss: 5.693558, G_cl: 2.309001, recon: 0.225809] time: 2:43:12.166251 \n","[Epoch 1/170] [Batch 400/448] [D_gan loss: 0.257414, acc_gan:  91%] [D_cl loss: 0.075257, acc_cl:   9%] [G_gan loss: 4.873608, G_cl: 2.453610, recon: 0.220260] time: 2:44:09.004642 \n","*** TEST *** [D_gan accuracy : 0.40625 ] [D_cl accuracy : 0.4375 ]\n","[Epoch 1/170] [Batch 405/448] [D_gan loss: 0.221684, acc_gan:  92%] [D_cl loss: 0.075284, acc_cl:   9%] [G_gan loss: 3.630774, G_cl: 2.757180, recon: 0.222156] time: 2:45:06.580373 \n","[Epoch 1/170] [Batch 410/448] [D_gan loss: 0.088328, acc_gan:  97%] [D_cl loss: 0.062956, acc_cl:   9%] [G_gan loss: 4.156179, G_cl: 2.501244, recon: 0.222199] time: 2:46:04.440521 \n","[Epoch 1/170] [Batch 415/448] [D_gan loss: 0.075745, acc_gan:  98%] [D_cl loss: 0.059649, acc_cl:   7%] [G_gan loss: 3.908289, G_cl: 2.548623, recon: 0.226198] time: 2:47:01.405487 \n","[Epoch 1/170] [Batch 420/448] [D_gan loss: 0.028796, acc_gan:  99%] [D_cl loss: 0.053423, acc_cl:   6%] [G_gan loss: 5.277055, G_cl: 2.383875, recon: 0.215762] time: 2:48:00.630058 \n","[Epoch 1/170] [Batch 425/448] [D_gan loss: 0.023631, acc_gan:  99%] [D_cl loss: 0.049535, acc_cl:  10%] [G_gan loss: 5.362936, G_cl: 2.269147, recon: 0.211957] time: 2:48:57.579566 \n","[Epoch 1/170] [Batch 430/448] [D_gan loss: 0.022099, acc_gan:  99%] [D_cl loss: 0.049309, acc_cl:  12%] [G_gan loss: 5.898659, G_cl: 2.236779, recon: 0.223992] time: 2:49:54.393452 \n","[Epoch 1/170] [Batch 435/448] [D_gan loss: 0.016239, acc_gan: 100%] [D_cl loss: 0.052760, acc_cl:   9%] [G_gan loss: 6.087272, G_cl: 2.210466, recon: 0.231393] time: 2:50:53.464877 \n","[Epoch 1/170] [Batch 440/448] [D_gan loss: 0.062001, acc_gan:  98%] [D_cl loss: 0.059915, acc_cl:  14%] [G_gan loss: 5.562271, G_cl: 2.263622, recon: 0.231146] time: 2:51:51.251568 \n","[Epoch 1/170] [Batch 445/448] [D_gan loss: 0.047901, acc_gan:  98%] [D_cl loss: 0.053986, acc_cl:   5%] [G_gan loss: 5.510319, G_cl: 2.347167, recon: 0.241564] time: 2:52:48.329882 \n","[Epoch 2/170] [Batch 0/448] [D_gan loss: 0.280278, acc_gan:  91%] [D_cl loss: 0.076528, acc_cl:  16%] [G_gan loss: 4.051290, G_cl: 2.607064, recon: 0.238587] time: 2:53:28.094945 \n","*** TEST *** [D_gan accuracy : 0.390625 ] [D_cl accuracy : 0.28125 ]\n","[Epoch 2/170] [Batch 5/448] [D_gan loss: 0.176862, acc_gan:  93%] [D_cl loss: 0.066670, acc_cl:  35%] [G_gan loss: 3.762544, G_cl: 2.611398, recon: 0.230547] time: 2:54:27.959110 \n","[Epoch 2/170] [Batch 10/448] [D_gan loss: 0.069586, acc_gan:  98%] [D_cl loss: 0.057523, acc_cl:  13%] [G_gan loss: 4.506667, G_cl: 2.460543, recon: 0.222506] time: 2:55:24.546609 \n","[Epoch 2/170] [Batch 15/448] [D_gan loss: 0.039393, acc_gan:  99%] [D_cl loss: 0.056915, acc_cl:  10%] [G_gan loss: 4.962378, G_cl: 2.367343, recon: 0.242235] time: 2:56:19.896807 \n","[Epoch 2/170] [Batch 20/448] [D_gan loss: 0.120609, acc_gan:  96%] [D_cl loss: 0.063429, acc_cl:  18%] [G_gan loss: 4.497721, G_cl: 2.363903, recon: 0.250724] time: 2:57:17.794184 \n","[Epoch 2/170] [Batch 25/448] [D_gan loss: 0.044454, acc_gan:  98%] [D_cl loss: 0.054808, acc_cl:  10%] [G_gan loss: 4.757782, G_cl: 2.351122, recon: 0.236109] time: 2:58:14.305985 \n","[Epoch 2/170] [Batch 30/448] [D_gan loss: 0.032116, acc_gan:  98%] [D_cl loss: 0.053586, acc_cl:  12%] [G_gan loss: 5.666277, G_cl: 2.286524, recon: 0.219344] time: 2:59:11.137477 \n","[Epoch 2/170] [Batch 35/448] [D_gan loss: 0.072720, acc_gan:  97%] [D_cl loss: 0.055489, acc_cl:  17%] [G_gan loss: 4.803276, G_cl: 2.456695, recon: 0.208299] time: 3:00:08.923019 \n","[Epoch 2/170] [Batch 40/448] [D_gan loss: 0.021056, acc_gan:  99%] [D_cl loss: 0.048266, acc_cl:   8%] [G_gan loss: 5.708436, G_cl: 2.255740, recon: 0.208299] time: 3:01:07.057281 \n","[Epoch 2/170] [Batch 45/448] [D_gan loss: 0.013185, acc_gan:  99%] [D_cl loss: 0.050952, acc_cl:  10%] [G_gan loss: 6.471312, G_cl: 2.162523, recon: 0.209448] time: 3:02:03.976219 \n","[Epoch 2/170] [Batch 50/448] [D_gan loss: 0.034493, acc_gan:  97%] [D_cl loss: 0.055230, acc_cl:   7%] [G_gan loss: 6.768320, G_cl: 2.156072, recon: 0.205598] time: 3:02:59.690186 \n","[Epoch 2/170] [Batch 55/448] [D_gan loss: 0.276177, acc_gan:  86%] [D_cl loss: 0.070349, acc_cl:  14%] [G_gan loss: 4.355591, G_cl: 2.430206, recon: 0.190808] time: 3:03:56.438193 \n","[Epoch 2/170] [Batch 60/448] [D_gan loss: 0.039791, acc_gan:  98%] [D_cl loss: 0.057115, acc_cl:  15%] [G_gan loss: 5.385219, G_cl: 2.277258, recon: 0.201300] time: 3:04:54.259719 \n","[Epoch 2/170] [Batch 65/448] [D_gan loss: 0.023175, acc_gan:  99%] [D_cl loss: 0.053311, acc_cl:  37%] [G_gan loss: 5.934013, G_cl: 2.304455, recon: 0.227888] time: 3:05:51.228991 \n","[Epoch 2/170] [Batch 70/448] [D_gan loss: 0.089099, acc_gan:  97%] [D_cl loss: 0.059198, acc_cl:  22%] [G_gan loss: 4.136242, G_cl: 2.450356, recon: 0.228186] time: 3:06:47.934442 \n","[Epoch 2/170] [Batch 75/448] [D_gan loss: 0.038303, acc_gan:  98%] [D_cl loss: 0.057058, acc_cl:  20%] [G_gan loss: 4.928599, G_cl: 2.359913, recon: 0.224150] time: 3:07:45.624311 \n","[Epoch 2/170] [Batch 80/448] [D_gan loss: 0.026863, acc_gan:  99%] [D_cl loss: 0.058114, acc_cl:  20%] [G_gan loss: 5.573624, G_cl: 2.220481, recon: 0.227330] time: 3:08:42.468044 \n","[Epoch 2/170] [Batch 85/448] [D_gan loss: 0.007333, acc_gan: 100%] [D_cl loss: 0.049998, acc_cl:  30%] [G_gan loss: 6.142033, G_cl: 2.217845, recon: 0.241711] time: 3:09:40.892680 \n","[Epoch 2/170] [Batch 90/448] [D_gan loss: 0.005957, acc_gan: 100%] [D_cl loss: 0.052319, acc_cl:  19%] [G_gan loss: 6.231417, G_cl: 2.219581, recon: 0.231784] time: 3:10:37.594970 \n","[Epoch 2/170] [Batch 95/448] [D_gan loss: 0.013605, acc_gan:  99%] [D_cl loss: 0.045781, acc_cl:  13%] [G_gan loss: 6.488914, G_cl: 2.277083, recon: 0.219108] time: 3:11:34.333123 \n","[Epoch 2/170] [Batch 100/448] [D_gan loss: 0.015819, acc_gan:  99%] [D_cl loss: 0.049649, acc_cl:  17%] [G_gan loss: 6.060869, G_cl: 2.242834, recon: 0.227285] time: 3:12:29.971303 \n","[Epoch 2/170] [Batch 105/448] [D_gan loss: 0.048794, acc_gan:  98%] [D_cl loss: 0.055590, acc_cl:  10%] [G_gan loss: 5.985142, G_cl: 2.268924, recon: 0.236947] time: 3:13:27.821116 \n","[Epoch 2/170] [Batch 110/448] [D_gan loss: 0.177634, acc_gan:  93%] [D_cl loss: 0.073937, acc_cl:  16%] [G_gan loss: 3.502602, G_cl: 2.469480, recon: 0.228928] time: 3:14:25.590918 \n","[Epoch 2/170] [Batch 115/448] [D_gan loss: 0.096258, acc_gan:  97%] [D_cl loss: 0.066207, acc_cl:  18%] [G_gan loss: 3.998734, G_cl: 2.328006, recon: 0.222875] time: 3:15:20.903465 \n","[Epoch 2/170] [Batch 120/448] [D_gan loss: 0.099176, acc_gan:  97%] [D_cl loss: 0.062187, acc_cl:  10%] [G_gan loss: 4.289909, G_cl: 2.376669, recon: 0.234861] time: 3:16:17.604655 \n","[Epoch 2/170] [Batch 125/448] [D_gan loss: 0.062267, acc_gan:  97%] [D_cl loss: 0.055151, acc_cl:   6%] [G_gan loss: 5.105211, G_cl: 2.259863, recon: 0.221853] time: 3:17:14.308218 \n","[Epoch 2/170] [Batch 130/448] [D_gan loss: 0.072131, acc_gan:  96%] [D_cl loss: 0.055745, acc_cl:   9%] [G_gan loss: 4.963730, G_cl: 2.315330, recon: 0.228840] time: 3:18:11.139065 \n","[Epoch 2/170] [Batch 135/448] [D_gan loss: 0.077319, acc_gan:  97%] [D_cl loss: 0.059771, acc_cl:  12%] [G_gan loss: 4.186321, G_cl: 2.393720, recon: 0.200770] time: 3:19:06.757020 \n","[Epoch 2/170] [Batch 140/448] [D_gan loss: 0.133240, acc_gan:  94%] [D_cl loss: 0.059610, acc_cl:  21%] [G_gan loss: 4.276803, G_cl: 2.377520, recon: 0.217545] time: 3:20:02.224951 \n","[Epoch 2/170] [Batch 145/448] [D_gan loss: 0.112565, acc_gan:  95%] [D_cl loss: 0.059070, acc_cl:   9%] [G_gan loss: 4.236768, G_cl: 2.432837, recon: 0.206920] time: 3:20:58.830532 \n","[Epoch 2/170] [Batch 150/448] [D_gan loss: 0.059122, acc_gan:  97%] [D_cl loss: 0.055404, acc_cl:  18%] [G_gan loss: 4.918084, G_cl: 2.323970, recon: 0.196810] time: 3:21:57.995369 \n","[Epoch 2/170] [Batch 155/448] [D_gan loss: 0.026425, acc_gan:  99%] [D_cl loss: 0.051218, acc_cl:  11%] [G_gan loss: 5.623268, G_cl: 2.335827, recon: 0.206957] time: 3:22:56.759882 \n","[Epoch 2/170] [Batch 160/448] [D_gan loss: 0.203600, acc_gan:  95%] [D_cl loss: 0.073705, acc_cl:   5%] [G_gan loss: 3.468490, G_cl: 2.438649, recon: 0.212531] time: 3:23:57.072799 \n","[Epoch 2/170] [Batch 165/448] [D_gan loss: 0.124507, acc_gan:  95%] [D_cl loss: 0.063022, acc_cl:   8%] [G_gan loss: 3.665704, G_cl: 2.442546, recon: 0.214351] time: 3:24:53.761190 \n","[Epoch 2/170] [Batch 170/448] [D_gan loss: 0.103846, acc_gan:  96%] [D_cl loss: 0.059155, acc_cl:  11%] [G_gan loss: 3.939204, G_cl: 2.353782, recon: 0.200120] time: 3:25:47.892386 \n","[Epoch 2/170] [Batch 175/448] [D_gan loss: 0.054276, acc_gan:  97%] [D_cl loss: 0.058873, acc_cl:  27%] [G_gan loss: 4.354150, G_cl: 2.283756, recon: 0.205043] time: 3:26:45.514613 \n","[Epoch 2/170] [Batch 180/448] [D_gan loss: 0.030707, acc_gan:  98%] [D_cl loss: 0.057300, acc_cl:  12%] [G_gan loss: 6.147973, G_cl: 2.335331, recon: 0.207384] time: 3:27:42.243477 \n","[Epoch 2/170] [Batch 185/448] [D_gan loss: 0.021301, acc_gan:  99%] [D_cl loss: 0.050754, acc_cl:  10%] [G_gan loss: 6.291689, G_cl: 2.244817, recon: 0.207849] time: 3:28:40.090302 \n","[Epoch 2/170] [Batch 190/448] [D_gan loss: 0.150734, acc_gan:  94%] [D_cl loss: 0.070994, acc_cl:  16%] [G_gan loss: 5.083562, G_cl: 2.315826, recon: 0.207134] time: 3:29:38.982719 \n","[Epoch 2/170] [Batch 195/448] [D_gan loss: 0.031084, acc_gan:  98%] [D_cl loss: 0.055645, acc_cl:  16%] [G_gan loss: 5.124329, G_cl: 2.239043, recon: 0.215484] time: 3:30:34.371938 \n","[Epoch 2/170] [Batch 200/448] [D_gan loss: 0.020862, acc_gan:  99%] [D_cl loss: 0.055273, acc_cl:   7%] [G_gan loss: 5.519994, G_cl: 2.197847, recon: 0.216738] time: 3:31:32.149035 \n","*** TEST *** [D_gan accuracy : 1.0 ] [D_cl accuracy : 0.453125 ]\n","[Epoch 2/170] [Batch 205/448] [D_gan loss: 0.011637, acc_gan: 100%] [D_cl loss: 0.045718, acc_cl:  29%] [G_gan loss: 5.898027, G_cl: 2.192444, recon: 0.229224] time: 3:32:29.572753 \n","[Epoch 2/170] [Batch 210/448] [D_gan loss: 0.032914, acc_gan:  99%] [D_cl loss: 0.050851, acc_cl:  33%] [G_gan loss: 5.615213, G_cl: 2.284247, recon: 0.231729] time: 3:33:26.149211 \n","[Epoch 2/170] [Batch 215/448] [D_gan loss: 0.070576, acc_gan:  96%] [D_cl loss: 0.059471, acc_cl:  10%] [G_gan loss: 3.959260, G_cl: 2.355640, recon: 0.241363] time: 3:34:21.610808 \n","[Epoch 2/170] [Batch 220/448] [D_gan loss: 0.067703, acc_gan:  98%] [D_cl loss: 0.060923, acc_cl:  12%] [G_gan loss: 4.697678, G_cl: 2.420182, recon: 0.229815] time: 3:35:19.177863 \n","[Epoch 2/170] [Batch 225/448] [D_gan loss: 0.097681, acc_gan:  95%] [D_cl loss: 0.057874, acc_cl:   9%] [G_gan loss: 3.654682, G_cl: 2.473010, recon: 0.203075] time: 3:36:17.110170 \n","[Epoch 2/170] [Batch 230/448] [D_gan loss: 0.094395, acc_gan:  95%] [D_cl loss: 0.061724, acc_cl:  20%] [G_gan loss: 3.973796, G_cl: 2.481616, recon: 0.194134] time: 3:37:15.901158 \n","[Epoch 2/170] [Batch 235/448] [D_gan loss: 0.022903, acc_gan:  99%] [D_cl loss: 0.049372, acc_cl:   9%] [G_gan loss: 5.572155, G_cl: 2.348815, recon: 0.202284] time: 3:38:15.979685 \n","[Epoch 2/170] [Batch 240/448] [D_gan loss: 0.024720, acc_gan:  99%] [D_cl loss: 0.050914, acc_cl:   7%] [G_gan loss: 6.258427, G_cl: 2.268487, recon: 0.198636] time: 3:39:14.813580 \n","[Epoch 2/170] [Batch 245/448] [D_gan loss: 0.397311, acc_gan:  88%] [D_cl loss: 0.099563, acc_cl:   6%] [G_gan loss: 1.974120, G_cl: 2.637559, recon: 0.192261] time: 3:40:13.346133 \n","[Epoch 2/170] [Batch 250/448] [D_gan loss: 0.238043, acc_gan:  91%] [D_cl loss: 0.078642, acc_cl:   4%] [G_gan loss: 2.679976, G_cl: 2.528322, recon: 0.198742] time: 3:41:10.030282 \n","[Epoch 2/170] [Batch 255/448] [D_gan loss: 0.195504, acc_gan:  93%] [D_cl loss: 0.082491, acc_cl:  15%] [G_gan loss: 3.056457, G_cl: 2.379791, recon: 0.194978] time: 3:42:06.560737 \n","[Epoch 2/170] [Batch 260/448] [D_gan loss: 0.179314, acc_gan:  93%] [D_cl loss: 0.069243, acc_cl:   7%] [G_gan loss: 3.313829, G_cl: 2.521158, recon: 0.191355] time: 3:43:02.121921 \n","[Epoch 2/170] [Batch 265/448] [D_gan loss: 0.328963, acc_gan:  89%] [D_cl loss: 0.082107, acc_cl:  14%] [G_gan loss: 2.684999, G_cl: 2.585704, recon: 0.181025] time: 3:43:59.920120 \n","[Epoch 2/170] [Batch 270/448] [D_gan loss: 0.210204, acc_gan:  92%] [D_cl loss: 0.078800, acc_cl:   7%] [G_gan loss: 2.883974, G_cl: 2.364703, recon: 0.184251] time: 3:44:55.464199 \n","[Epoch 2/170] [Batch 275/448] [D_gan loss: 0.274369, acc_gan:  90%] [D_cl loss: 0.083746, acc_cl:   7%] [G_gan loss: 2.643435, G_cl: 2.551988, recon: 0.192020] time: 3:45:51.117498 \n","[Epoch 2/170] [Batch 280/448] [D_gan loss: 0.228579, acc_gan:  92%] [D_cl loss: 0.079300, acc_cl:  15%] [G_gan loss: 3.019110, G_cl: 2.522694, recon: 0.191372] time: 3:46:48.960878 \n","[Epoch 2/170] [Batch 285/448] [D_gan loss: 0.253739, acc_gan:  89%] [D_cl loss: 0.078052, acc_cl:  15%] [G_gan loss: 2.867947, G_cl: 2.458341, recon: 0.177151] time: 3:47:46.657853 \n","[Epoch 2/170] [Batch 290/448] [D_gan loss: 0.209157, acc_gan:  92%] [D_cl loss: 0.066375, acc_cl:   8%] [G_gan loss: 2.841682, G_cl: 2.564053, recon: 0.194339] time: 3:48:44.392449 \n","[Epoch 2/170] [Batch 295/448] [D_gan loss: 0.146431, acc_gan:  93%] [D_cl loss: 0.064835, acc_cl:  14%] [G_gan loss: 3.724458, G_cl: 2.337846, recon: 0.183074] time: 3:49:42.265396 \n","[Epoch 2/170] [Batch 300/448] [D_gan loss: 0.223677, acc_gan:  92%] [D_cl loss: 0.068392, acc_cl:  26%] [G_gan loss: 3.431102, G_cl: 2.418595, recon: 0.189017] time: 3:50:40.145559 \n","[Epoch 2/170] [Batch 305/448] [D_gan loss: 0.167191, acc_gan:  93%] [D_cl loss: 0.065723, acc_cl:  14%] [G_gan loss: 3.462249, G_cl: 2.519019, recon: 0.194891] time: 3:51:35.691480 \n","[Epoch 2/170] [Batch 310/448] [D_gan loss: 0.058063, acc_gan:  98%] [D_cl loss: 0.055784, acc_cl:  12%] [G_gan loss: 4.189798, G_cl: 2.380898, recon: 0.209795] time: 3:52:31.793909 \n","[Epoch 2/170] [Batch 315/448] [D_gan loss: 0.074210, acc_gan:  97%] [D_cl loss: 0.056154, acc_cl:  17%] [G_gan loss: 4.320269, G_cl: 2.396057, recon: 0.203855] time: 3:53:29.458485 \n","[Epoch 2/170] [Batch 320/448] [D_gan loss: 0.045561, acc_gan:  98%] [D_cl loss: 0.049417, acc_cl:   9%] [G_gan loss: 4.869838, G_cl: 2.345063, recon: 0.213389] time: 3:54:27.129662 \n","[Epoch 2/170] [Batch 325/448] [D_gan loss: 0.046056, acc_gan:  97%] [D_cl loss: 0.053544, acc_cl:  22%] [G_gan loss: 5.408140, G_cl: 2.282648, recon: 0.217421] time: 3:55:22.745954 \n","[Epoch 2/170] [Batch 330/448] [D_gan loss: 0.115366, acc_gan:  95%] [D_cl loss: 0.062831, acc_cl:  18%] [G_gan loss: 5.304802, G_cl: 2.216780, recon: 0.240567] time: 3:56:19.310435 \n","[Epoch 2/170] [Batch 335/448] [D_gan loss: 0.047102, acc_gan:  98%] [D_cl loss: 0.054555, acc_cl:  16%] [G_gan loss: 5.855443, G_cl: 2.341811, recon: 0.241219] time: 3:57:19.601630 \n","[Epoch 2/170] [Batch 340/448] [D_gan loss: 0.211064, acc_gan:  93%] [D_cl loss: 0.071660, acc_cl:  17%] [G_gan loss: 3.654034, G_cl: 2.458628, recon: 0.219857] time: 3:58:18.328297 \n","[Epoch 2/170] [Batch 345/448] [D_gan loss: 0.070471, acc_gan:  98%] [D_cl loss: 0.053317, acc_cl:  26%] [G_gan loss: 4.715363, G_cl: 2.375969, recon: 0.205303] time: 3:59:17.447865 \n","[Epoch 2/170] [Batch 350/448] [D_gan loss: 0.017867, acc_gan:  99%] [D_cl loss: 0.052902, acc_cl:  21%] [G_gan loss: 5.597853, G_cl: 2.256696, recon: 0.194275] time: 4:00:16.500281 \n","[Epoch 2/170] [Batch 355/448] [D_gan loss: 0.022576, acc_gan:  99%] [D_cl loss: 0.057251, acc_cl:  33%] [G_gan loss: 5.631197, G_cl: 2.220993, recon: 0.201030] time: 4:01:14.164956 \n","[Epoch 2/170] [Batch 360/448] [D_gan loss: 0.019926, acc_gan:  99%] [D_cl loss: 0.055439, acc_cl:   8%] [G_gan loss: 6.121164, G_cl: 2.266757, recon: 0.189091] time: 4:02:12.026197 \n","[Epoch 2/170] [Batch 365/448] [D_gan loss: 0.013481, acc_gan:  99%] [D_cl loss: 0.048288, acc_cl:   7%] [G_gan loss: 6.366687, G_cl: 2.237432, recon: 0.183071] time: 4:03:10.960939 \n","[Epoch 2/170] [Batch 370/448] [D_gan loss: 0.003840, acc_gan: 100%] [D_cl loss: 0.050862, acc_cl:   7%] [G_gan loss: 6.689709, G_cl: 2.215689, recon: 0.178129] time: 4:04:08.820248 \n","[Epoch 2/170] [Batch 375/448] [D_gan loss: 0.037019, acc_gan:  98%] [D_cl loss: 0.050886, acc_cl:  10%] [G_gan loss: 7.126036, G_cl: 2.167902, recon: 0.181143] time: 4:05:06.796566 \n","[Epoch 2/170] [Batch 380/448] [D_gan loss: 0.018473, acc_gan:  99%] [D_cl loss: 0.051446, acc_cl:  18%] [G_gan loss: 6.611365, G_cl: 2.215761, recon: 0.185396] time: 4:06:04.684842 \n","[Epoch 2/170] [Batch 385/448] [D_gan loss: 0.057060, acc_gan:  98%] [D_cl loss: 0.059407, acc_cl:  26%] [G_gan loss: 6.058101, G_cl: 2.298929, recon: 0.191925] time: 4:07:02.815661 \n","[Epoch 2/170] [Batch 390/448] [D_gan loss: 0.316811, acc_gan:  87%] [D_cl loss: 0.091608, acc_cl:   5%] [G_gan loss: 2.211764, G_cl: 2.633633, recon: 0.196687] time: 4:08:01.836081 \n","[Epoch 2/170] [Batch 395/448] [D_gan loss: 0.113219, acc_gan:  96%] [D_cl loss: 0.065447, acc_cl:   4%] [G_gan loss: 3.396472, G_cl: 2.274512, recon: 0.198214] time: 4:08:59.967251 \n","[Epoch 2/170] [Batch 400/448] [D_gan loss: 0.014219, acc_gan:  99%] [D_cl loss: 0.048678, acc_cl:   7%] [G_gan loss: 5.006422, G_cl: 2.211418, recon: 0.192961] time: 4:09:56.839855 \n","*** TEST *** [D_gan accuracy : 0.984375 ] [D_cl accuracy : 0.359375 ]\n","[Epoch 2/170] [Batch 405/448] [D_gan loss: 0.006005, acc_gan: 100%] [D_cl loss: 0.049427, acc_cl:  16%] [G_gan loss: 5.778906, G_cl: 2.179605, recon: 0.182991] time: 4:10:54.405227 \n","[Epoch 2/170] [Batch 410/448] [D_gan loss: 0.026781, acc_gan:  99%] [D_cl loss: 0.053439, acc_cl:  35%] [G_gan loss: 5.547085, G_cl: 2.256788, recon: 0.192063] time: 4:11:49.943021 \n","[Epoch 2/170] [Batch 415/448] [D_gan loss: 0.085972, acc_gan:  97%] [D_cl loss: 0.061503, acc_cl:  22%] [G_gan loss: 5.422873, G_cl: 2.337003, recon: 0.216240] time: 4:12:46.356086 \n","[Epoch 2/170] [Batch 420/448] [D_gan loss: 0.049690, acc_gan:  98%] [D_cl loss: 0.048792, acc_cl:  34%] [G_gan loss: 5.995818, G_cl: 2.348633, recon: 0.213646] time: 4:13:41.873541 \n","[Epoch 2/170] [Batch 425/448] [D_gan loss: 0.082326, acc_gan:  96%] [D_cl loss: 0.058714, acc_cl:  20%] [G_gan loss: 5.632612, G_cl: 2.348887, recon: 0.218477] time: 4:14:37.243986 \n","[Epoch 2/170] [Batch 430/448] [D_gan loss: 0.009084, acc_gan: 100%] [D_cl loss: 0.050272, acc_cl:   9%] [G_gan loss: 6.542668, G_cl: 2.231354, recon: 0.221614] time: 4:15:35.124800 \n","[Epoch 2/170] [Batch 435/448] [D_gan loss: 0.014476, acc_gan:  99%] [D_cl loss: 0.047663, acc_cl:  15%] [G_gan loss: 6.275321, G_cl: 2.281292, recon: 0.221256] time: 4:16:32.840223 \n","[Epoch 2/170] [Batch 440/448] [D_gan loss: 0.069932, acc_gan:  97%] [D_cl loss: 0.057809, acc_cl:  36%] [G_gan loss: 5.775344, G_cl: 2.367069, recon: 0.233310] time: 4:17:29.558937 \n","[Epoch 2/170] [Batch 445/448] [D_gan loss: 0.063902, acc_gan:  97%] [D_cl loss: 0.051044, acc_cl:  25%] [G_gan loss: 5.140367, G_cl: 2.483015, recon: 0.225177] time: 4:18:25.074799 \n","[Epoch 3/170] [Batch 0/448] [D_gan loss: 0.031562, acc_gan:  99%] [D_cl loss: 0.051541, acc_cl:   5%] [G_gan loss: 5.818526, G_cl: 2.300865, recon: 0.226754] time: 4:19:04.579223 \n","*** TEST *** [D_gan accuracy : 0.9375 ] [D_cl accuracy : 0.515625 ]\n","[Epoch 3/170] [Batch 5/448] [D_gan loss: 0.018633, acc_gan:  99%] [D_cl loss: 0.048604, acc_cl:   9%] [G_gan loss: 6.249800, G_cl: 2.231012, recon: 0.227589] time: 4:20:05.583730 \n","[Epoch 3/170] [Batch 10/448] [D_gan loss: 0.008365, acc_gan:  99%] [D_cl loss: 0.044598, acc_cl:  23%] [G_gan loss: 6.672672, G_cl: 2.200945, recon: 0.228786] time: 4:21:04.530140 \n","[Epoch 3/170] [Batch 15/448] [D_gan loss: 0.005738, acc_gan: 100%] [D_cl loss: 0.052226, acc_cl:  10%] [G_gan loss: 6.620155, G_cl: 2.297789, recon: 0.223789] time: 4:22:01.505326 \n","[Epoch 3/170] [Batch 20/448] [D_gan loss: 0.017771, acc_gan:  98%] [D_cl loss: 0.047228, acc_cl:  10%] [G_gan loss: 6.768270, G_cl: 2.272806, recon: 0.210032] time: 4:22:56.819445 \n","[Epoch 3/170] [Batch 25/448] [D_gan loss: 0.046945, acc_gan:  97%] [D_cl loss: 0.054579, acc_cl:   9%] [G_gan loss: 6.346830, G_cl: 2.385365, recon: 0.208851] time: 4:23:54.751251 \n","[Epoch 3/170] [Batch 30/448] [D_gan loss: 0.104912, acc_gan:  97%] [D_cl loss: 0.057747, acc_cl:   8%] [G_gan loss: 3.738107, G_cl: 2.560530, recon: 0.208240] time: 4:24:50.448918 \n","[Epoch 3/170] [Batch 35/448] [D_gan loss: 0.052177, acc_gan:  98%] [D_cl loss: 0.051110, acc_cl:   6%] [G_gan loss: 4.675613, G_cl: 2.503845, recon: 0.199040] time: 4:25:45.923190 \n","[Epoch 3/170] [Batch 40/448] [D_gan loss: 0.059561, acc_gan:  97%] [D_cl loss: 0.056464, acc_cl:  20%] [G_gan loss: 5.291594, G_cl: 2.422601, recon: 0.205575] time: 4:26:44.763616 \n","[Epoch 3/170] [Batch 45/448] [D_gan loss: 0.054410, acc_gan:  98%] [D_cl loss: 0.052522, acc_cl:  18%] [G_gan loss: 5.215054, G_cl: 2.378804, recon: 0.209223] time: 4:27:41.476361 \n","[Epoch 3/170] [Batch 50/448] [D_gan loss: 0.018939, acc_gan:  99%] [D_cl loss: 0.047507, acc_cl:  12%] [G_gan loss: 5.258433, G_cl: 2.426941, recon: 0.211979] time: 4:28:37.003306 \n","[Epoch 3/170] [Batch 55/448] [D_gan loss: 0.027150, acc_gan:  98%] [D_cl loss: 0.048679, acc_cl:  22%] [G_gan loss: 6.483018, G_cl: 2.371874, recon: 0.235246] time: 4:29:33.642169 \n","[Epoch 3/170] [Batch 60/448] [D_gan loss: 0.163562, acc_gan:  94%] [D_cl loss: 0.064959, acc_cl:  11%] [G_gan loss: 4.057795, G_cl: 2.575940, recon: 0.271609] time: 4:30:30.373816 \n","[Epoch 3/170] [Batch 65/448] [D_gan loss: 0.029699, acc_gan:  99%] [D_cl loss: 0.048674, acc_cl:   7%] [G_gan loss: 4.936161, G_cl: 2.392379, recon: 0.245913] time: 4:31:28.354165 \n","[Epoch 3/170] [Batch 70/448] [D_gan loss: 0.036681, acc_gan:  98%] [D_cl loss: 0.052618, acc_cl:   7%] [G_gan loss: 5.257953, G_cl: 2.324034, recon: 0.251140] time: 4:32:26.127327 \n","[Epoch 3/170] [Batch 75/448] [D_gan loss: 0.053550, acc_gan:  98%] [D_cl loss: 0.059028, acc_cl:  10%] [G_gan loss: 4.284265, G_cl: 2.342652, recon: 0.227450] time: 4:33:23.878171 \n","[Epoch 3/170] [Batch 80/448] [D_gan loss: 0.009516, acc_gan:  99%] [D_cl loss: 0.047489, acc_cl:   7%] [G_gan loss: 6.721500, G_cl: 2.333655, recon: 0.216460] time: 4:34:19.625496 \n","[Epoch 3/170] [Batch 85/448] [D_gan loss: 0.013494, acc_gan:  99%] [D_cl loss: 0.044843, acc_cl:  10%] [G_gan loss: 6.654709, G_cl: 2.362361, recon: 0.206627] time: 4:35:18.460068 \n","[Epoch 3/170] [Batch 90/448] [D_gan loss: 0.083577, acc_gan:  96%] [D_cl loss: 0.055906, acc_cl:   8%] [G_gan loss: 4.648411, G_cl: 2.431512, recon: 0.220127] time: 4:36:15.080121 \n","[Epoch 3/170] [Batch 95/448] [D_gan loss: 0.064904, acc_gan:  97%] [D_cl loss: 0.055270, acc_cl:  13%] [G_gan loss: 5.741117, G_cl: 2.292801, recon: 0.212120] time: 4:37:11.669806 \n","[Epoch 3/170] [Batch 100/448] [D_gan loss: 0.020581, acc_gan:  99%] [D_cl loss: 0.051199, acc_cl:  12%] [G_gan loss: 5.756079, G_cl: 2.249745, recon: 0.226533] time: 4:38:09.435689 \n","[Epoch 3/170] [Batch 105/448] [D_gan loss: 0.060825, acc_gan:  98%] [D_cl loss: 0.051472, acc_cl:  16%] [G_gan loss: 4.368542, G_cl: 2.391015, recon: 0.223427] time: 4:39:06.105939 \n","[Epoch 3/170] [Batch 110/448] [D_gan loss: 0.035959, acc_gan:  98%] [D_cl loss: 0.050974, acc_cl:  15%] [G_gan loss: 5.333576, G_cl: 2.440558, recon: 0.220593] time: 4:40:01.797447 \n","[Epoch 3/170] [Batch 115/448] [D_gan loss: 0.074877, acc_gan:  96%] [D_cl loss: 0.052152, acc_cl:  12%] [G_gan loss: 4.782245, G_cl: 2.460975, recon: 0.212525] time: 4:40:59.909341 \n","[Epoch 3/170] [Batch 120/448] [D_gan loss: 0.081163, acc_gan:  98%] [D_cl loss: 0.051251, acc_cl:  15%] [G_gan loss: 5.615965, G_cl: 2.396704, recon: 0.215911] time: 4:41:54.291888 \n","[Epoch 3/170] [Batch 125/448] [D_gan loss: 0.030006, acc_gan:  99%] [D_cl loss: 0.048625, acc_cl:   9%] [G_gan loss: 5.391585, G_cl: 2.423872, recon: 0.216492] time: 4:42:50.880669 \n","[Epoch 3/170] [Batch 130/448] [D_gan loss: 0.109423, acc_gan:  95%] [D_cl loss: 0.056746, acc_cl:  11%] [G_gan loss: 4.766026, G_cl: 2.425532, recon: 0.212145] time: 4:43:47.820144 \n","[Epoch 3/170] [Batch 135/448] [D_gan loss: 0.063737, acc_gan:  97%] [D_cl loss: 0.052571, acc_cl:  19%] [G_gan loss: 5.852814, G_cl: 2.301141, recon: 0.215828] time: 4:44:43.329222 \n","[Epoch 3/170] [Batch 140/448] [D_gan loss: 0.149013, acc_gan:  94%] [D_cl loss: 0.059116, acc_cl:  43%] [G_gan loss: 4.716195, G_cl: 2.394370, recon: 0.242948] time: 4:45:38.846242 \n","[Epoch 3/170] [Batch 145/448] [D_gan loss: 0.089389, acc_gan:  97%] [D_cl loss: 0.060217, acc_cl:  25%] [G_gan loss: 4.626409, G_cl: 2.508254, recon: 0.228171] time: 4:46:35.617454 \n","[Epoch 3/170] [Batch 150/448] [D_gan loss: 0.050257, acc_gan:  99%] [D_cl loss: 0.052024, acc_cl:  21%] [G_gan loss: 5.127286, G_cl: 2.413632, recon: 0.225522] time: 4:47:29.936358 \n","[Epoch 3/170] [Batch 155/448] [D_gan loss: 0.098714, acc_gan:  96%] [D_cl loss: 0.059021, acc_cl:  23%] [G_gan loss: 4.061115, G_cl: 2.489761, recon: 0.231031] time: 4:48:25.418948 \n","[Epoch 3/170] [Batch 160/448] [D_gan loss: 0.064991, acc_gan:  98%] [D_cl loss: 0.057246, acc_cl:  16%] [G_gan loss: 4.431544, G_cl: 2.530998, recon: 0.199938] time: 4:49:22.289232 \n","[Epoch 3/170] [Batch 165/448] [D_gan loss: 0.021118, acc_gan:  99%] [D_cl loss: 0.056048, acc_cl:  21%] [G_gan loss: 5.367441, G_cl: 2.391021, recon: 0.204022] time: 4:50:19.807954 \n","[Epoch 3/170] [Batch 170/448] [D_gan loss: 0.032892, acc_gan:  99%] [D_cl loss: 0.052695, acc_cl:  31%] [G_gan loss: 5.566933, G_cl: 2.332070, recon: 0.210465] time: 4:51:18.721480 \n","[Epoch 3/170] [Batch 175/448] [D_gan loss: 0.224905, acc_gan:  93%] [D_cl loss: 0.078816, acc_cl:   4%] [G_gan loss: 3.077630, G_cl: 2.526012, recon: 0.217054] time: 4:52:15.512794 \n","[Epoch 3/170] [Batch 180/448] [D_gan loss: 0.170682, acc_gan:  95%] [D_cl loss: 0.067973, acc_cl:   6%] [G_gan loss: 3.246439, G_cl: 2.616602, recon: 0.209066] time: 4:53:13.343042 \n","[Epoch 3/170] [Batch 185/448] [D_gan loss: 0.166306, acc_gan:  94%] [D_cl loss: 0.062271, acc_cl:   7%] [G_gan loss: 3.155692, G_cl: 2.613688, recon: 0.195928] time: 4:54:08.947512 \n","[Epoch 3/170] [Batch 190/448] [D_gan loss: 0.219148, acc_gan:  91%] [D_cl loss: 0.072164, acc_cl:  29%] [G_gan loss: 3.613516, G_cl: 2.614310, recon: 0.171752] time: 4:55:06.613013 \n","[Epoch 3/170] [Batch 195/448] [D_gan loss: 0.313836, acc_gan:  89%] [D_cl loss: 0.082844, acc_cl:   9%] [G_gan loss: 2.467475, G_cl: 2.487152, recon: 0.171685] time: 4:56:03.408557 \n","[Epoch 3/170] [Batch 200/448] [D_gan loss: 0.215084, acc_gan:  92%] [D_cl loss: 0.070332, acc_cl:  28%] [G_gan loss: 2.654714, G_cl: 2.405107, recon: 0.172651] time: 4:57:01.626969 \n","*** TEST *** [D_gan accuracy : 0.515625 ] [D_cl accuracy : 0.34375 ]\n","[Epoch 3/170] [Batch 205/448] [D_gan loss: 0.175573, acc_gan:  93%] [D_cl loss: 0.063433, acc_cl:   8%] [G_gan loss: 2.727988, G_cl: 2.446683, recon: 0.174969] time: 4:58:01.504177 \n","[Epoch 3/170] [Batch 210/448] [D_gan loss: 0.065756, acc_gan:  97%] [D_cl loss: 0.051480, acc_cl:  10%] [G_gan loss: 4.173976, G_cl: 2.341787, recon: 0.175641] time: 4:58:58.126652 \n","[Epoch 3/170] [Batch 215/448] [D_gan loss: 0.039190, acc_gan:  99%] [D_cl loss: 0.053247, acc_cl:  20%] [G_gan loss: 4.609967, G_cl: 2.352283, recon: 0.183779] time: 4:59:55.067691 \n","[Epoch 3/170] [Batch 220/448] [D_gan loss: 0.040848, acc_gan:  98%] [D_cl loss: 0.050333, acc_cl:  19%] [G_gan loss: 4.784223, G_cl: 2.398843, recon: 0.189592] time: 5:00:51.760235 \n","[Epoch 3/170] [Batch 225/448] [D_gan loss: 0.052248, acc_gan:  97%] [D_cl loss: 0.058012, acc_cl:  22%] [G_gan loss: 5.189481, G_cl: 2.446437, recon: 0.208552] time: 5:01:49.792818 \n","[Epoch 3/170] [Batch 230/448] [D_gan loss: 0.062199, acc_gan:  97%] [D_cl loss: 0.049650, acc_cl:  15%] [G_gan loss: 5.010977, G_cl: 2.394475, recon: 0.220580] time: 5:02:45.361027 \n","[Epoch 3/170] [Batch 235/448] [D_gan loss: 0.034274, acc_gan:  99%] [D_cl loss: 0.052293, acc_cl:  11%] [G_gan loss: 5.506653, G_cl: 2.435529, recon: 0.228410] time: 5:03:40.054128 \n","[Epoch 3/170] [Batch 240/448] [D_gan loss: 0.199447, acc_gan:  91%] [D_cl loss: 0.065339, acc_cl:   8%] [G_gan loss: 5.826957, G_cl: 2.338814, recon: 0.243908] time: 5:04:35.432253 \n","[Epoch 3/170] [Batch 245/448] [D_gan loss: 0.101097, acc_gan:  97%] [D_cl loss: 0.059441, acc_cl:   9%] [G_gan loss: 4.428015, G_cl: 2.490578, recon: 0.237318] time: 5:05:30.753195 \n","[Epoch 3/170] [Batch 250/448] [D_gan loss: 0.064739, acc_gan:  98%] [D_cl loss: 0.054993, acc_cl:  20%] [G_gan loss: 4.361985, G_cl: 2.426422, recon: 0.218696] time: 5:06:26.183898 \n","[Epoch 3/170] [Batch 255/448] [D_gan loss: 0.029979, acc_gan:  99%] [D_cl loss: 0.049346, acc_cl:  15%] [G_gan loss: 5.560793, G_cl: 2.259891, recon: 0.211527] time: 5:07:23.893051 \n","[Epoch 3/170] [Batch 260/448] [D_gan loss: 0.021898, acc_gan:  98%] [D_cl loss: 0.044450, acc_cl:  15%] [G_gan loss: 5.905138, G_cl: 2.257232, recon: 0.205130] time: 5:08:24.172327 \n","[Epoch 3/170] [Batch 265/448] [D_gan loss: 0.064698, acc_gan:  97%] [D_cl loss: 0.047520, acc_cl:  13%] [G_gan loss: 4.465139, G_cl: 2.531177, recon: 0.214561] time: 5:09:21.143061 \n","[Epoch 3/170] [Batch 270/448] [D_gan loss: 0.046224, acc_gan:  98%] [D_cl loss: 0.044583, acc_cl:  12%] [G_gan loss: 4.913247, G_cl: 2.357349, recon: 0.196598] time: 5:10:16.749248 \n","[Epoch 3/170] [Batch 275/448] [D_gan loss: 0.204293, acc_gan:  89%] [D_cl loss: 0.074266, acc_cl:  13%] [G_gan loss: 5.100712, G_cl: 2.386154, recon: 0.196144] time: 5:11:13.476050 \n","[Epoch 3/170] [Batch 280/448] [D_gan loss: 0.088905, acc_gan:  97%] [D_cl loss: 0.055516, acc_cl:   9%] [G_gan loss: 4.041173, G_cl: 2.569347, recon: 0.192425] time: 5:12:09.096267 \n","[Epoch 3/170] [Batch 285/448] [D_gan loss: 0.045774, acc_gan:  98%] [D_cl loss: 0.051562, acc_cl:  10%] [G_gan loss: 4.561622, G_cl: 2.417407, recon: 0.193295] time: 5:13:05.659324 \n","[Epoch 3/170] [Batch 290/448] [D_gan loss: 0.044149, acc_gan:  98%] [D_cl loss: 0.050567, acc_cl:  12%] [G_gan loss: 4.743374, G_cl: 2.465718, recon: 0.187518] time: 5:14:04.509420 \n","[Epoch 3/170] [Batch 295/448] [D_gan loss: 0.132448, acc_gan:  94%] [D_cl loss: 0.057319, acc_cl:  20%] [G_gan loss: 4.410978, G_cl: 2.586997, recon: 0.199372] time: 5:14:58.974831 \n","[Epoch 3/170] [Batch 300/448] [D_gan loss: 0.021271, acc_gan:  99%] [D_cl loss: 0.047670, acc_cl:   9%] [G_gan loss: 5.379687, G_cl: 2.380354, recon: 0.221359] time: 5:15:57.874211 \n","[Epoch 3/170] [Batch 305/448] [D_gan loss: 0.052457, acc_gan:  98%] [D_cl loss: 0.050403, acc_cl:  11%] [G_gan loss: 5.151214, G_cl: 2.409792, recon: 0.235204] time: 5:16:55.412805 \n","[Epoch 3/170] [Batch 310/448] [D_gan loss: 0.054820, acc_gan:  97%] [D_cl loss: 0.052636, acc_cl:  11%] [G_gan loss: 5.230186, G_cl: 2.460061, recon: 0.233445] time: 5:17:50.726096 \n","[Epoch 3/170] [Batch 315/448] [D_gan loss: 0.039419, acc_gan:  98%] [D_cl loss: 0.042397, acc_cl:  17%] [G_gan loss: 5.574931, G_cl: 2.315683, recon: 0.230804] time: 5:18:49.605350 \n","[Epoch 3/170] [Batch 320/448] [D_gan loss: 0.178645, acc_gan:  92%] [D_cl loss: 0.065576, acc_cl:  12%] [G_gan loss: 3.608458, G_cl: 2.504290, recon: 0.205405] time: 5:19:46.289879 \n","[Epoch 3/170] [Batch 325/448] [D_gan loss: 0.098469, acc_gan:  96%] [D_cl loss: 0.060325, acc_cl:  10%] [G_gan loss: 5.227056, G_cl: 2.230579, recon: 0.218285] time: 5:20:45.033193 \n","[Epoch 3/170] [Batch 330/448] [D_gan loss: 0.074818, acc_gan:  95%] [D_cl loss: 0.053458, acc_cl:   8%] [G_gan loss: 5.265957, G_cl: 2.333413, recon: 0.217239] time: 5:21:42.808944 \n","[Epoch 3/170] [Batch 335/448] [D_gan loss: 0.164286, acc_gan:  95%] [D_cl loss: 0.059400, acc_cl:  18%] [G_gan loss: 4.955937, G_cl: 2.463952, recon: 0.219430] time: 5:22:41.622259 \n","[Epoch 3/170] [Batch 340/448] [D_gan loss: 0.109303, acc_gan:  95%] [D_cl loss: 0.057284, acc_cl:  18%] [G_gan loss: 5.261633, G_cl: 2.434030, recon: 0.213311] time: 5:23:39.448154 \n","[Epoch 3/170] [Batch 345/448] [D_gan loss: 0.111482, acc_gan:  94%] [D_cl loss: 0.057695, acc_cl:   7%] [G_gan loss: 3.838943, G_cl: 2.401511, recon: 0.215515] time: 5:24:37.049162 \n","[Epoch 3/170] [Batch 350/448] [D_gan loss: 0.048729, acc_gan:  98%] [D_cl loss: 0.051662, acc_cl:   9%] [G_gan loss: 4.642821, G_cl: 2.479825, recon: 0.202391] time: 5:25:35.937725 \n","[Epoch 3/170] [Batch 355/448] [D_gan loss: 0.076832, acc_gan:  97%] [D_cl loss: 0.049539, acc_cl:  10%] [G_gan loss: 4.913559, G_cl: 2.392155, recon: 0.208161] time: 5:26:33.614025 \n","[Epoch 3/170] [Batch 360/448] [D_gan loss: 0.025421, acc_gan:  99%] [D_cl loss: 0.048608, acc_cl:  14%] [G_gan loss: 4.916600, G_cl: 2.337527, recon: 0.209865] time: 5:27:28.848810 \n","[Epoch 3/170] [Batch 365/448] [D_gan loss: 0.011569, acc_gan:  99%] [D_cl loss: 0.045934, acc_cl:  10%] [G_gan loss: 6.094937, G_cl: 2.244900, recon: 0.218697] time: 5:28:27.589135 \n","[Epoch 3/170] [Batch 370/448] [D_gan loss: 0.012270, acc_gan:  99%] [D_cl loss: 0.052088, acc_cl:  18%] [G_gan loss: 5.860121, G_cl: 2.279671, recon: 0.242085] time: 5:29:26.585937 \n","[Epoch 3/170] [Batch 375/448] [D_gan loss: 0.326778, acc_gan:  90%] [D_cl loss: 0.091244, acc_cl:   5%] [G_gan loss: 2.495982, G_cl: 2.951719, recon: 0.241021] time: 5:30:23.186386 \n","[Epoch 3/170] [Batch 380/448] [D_gan loss: 0.105674, acc_gan:  94%] [D_cl loss: 0.057816, acc_cl:   7%] [G_gan loss: 4.039813, G_cl: 2.303679, recon: 0.247154] time: 5:31:22.068278 \n","[Epoch 3/170] [Batch 385/448] [D_gan loss: 0.125528, acc_gan:  95%] [D_cl loss: 0.064234, acc_cl:   9%] [G_gan loss: 3.523081, G_cl: 2.323192, recon: 0.251922] time: 5:32:18.882394 \n","[Epoch 3/170] [Batch 390/448] [D_gan loss: 0.085662, acc_gan:  97%] [D_cl loss: 0.055976, acc_cl:  12%] [G_gan loss: 3.695687, G_cl: 2.380523, recon: 0.240407] time: 5:33:15.762380 \n","[Epoch 3/170] [Batch 395/448] [D_gan loss: 0.125634, acc_gan:  96%] [D_cl loss: 0.066660, acc_cl:   6%] [G_gan loss: 3.778683, G_cl: 2.490959, recon: 0.241974] time: 5:34:11.350396 \n","[Epoch 3/170] [Batch 400/448] [D_gan loss: 0.064265, acc_gan:  98%] [D_cl loss: 0.052789, acc_cl:  10%] [G_gan loss: 4.133194, G_cl: 2.396477, recon: 0.226291] time: 5:35:10.323096 \n","*** TEST *** [D_gan accuracy : 0.875 ] [D_cl accuracy : 0.359375 ]\n","[Epoch 3/170] [Batch 405/448] [D_gan loss: 0.267543, acc_gan:  90%] [D_cl loss: 0.078477, acc_cl:   8%] [G_gan loss: 3.249328, G_cl: 2.472818, recon: 0.210867] time: 5:36:08.173104 \n","[Epoch 3/170] [Batch 410/448] [D_gan loss: 0.095981, acc_gan:  96%] [D_cl loss: 0.059743, acc_cl:   7%] [G_gan loss: 4.142331, G_cl: 2.517183, recon: 0.222293] time: 5:37:04.857020 \n","[Epoch 3/170] [Batch 415/448] [D_gan loss: 0.160045, acc_gan:  94%] [D_cl loss: 0.059960, acc_cl:  15%] [G_gan loss: 3.644830, G_cl: 2.541638, recon: 0.205206] time: 5:38:01.411123 \n","[Epoch 3/170] [Batch 420/448] [D_gan loss: 0.106772, acc_gan:  96%] [D_cl loss: 0.055628, acc_cl:   8%] [G_gan loss: 3.728393, G_cl: 2.533668, recon: 0.201778] time: 5:38:59.299570 \n","[Epoch 3/170] [Batch 425/448] [D_gan loss: 0.107865, acc_gan:  95%] [D_cl loss: 0.061754, acc_cl:  17%] [G_gan loss: 4.024964, G_cl: 2.551193, recon: 0.180716] time: 5:39:59.372393 \n","[Epoch 3/170] [Batch 430/448] [D_gan loss: 0.169167, acc_gan:  94%] [D_cl loss: 0.066563, acc_cl:  12%] [G_gan loss: 3.101215, G_cl: 2.510499, recon: 0.175833] time: 5:40:57.350468 \n","[Epoch 3/170] [Batch 435/448] [D_gan loss: 0.133738, acc_gan:  95%] [D_cl loss: 0.055939, acc_cl:  10%] [G_gan loss: 3.415309, G_cl: 2.565819, recon: 0.175553] time: 5:41:53.922474 \n","[Epoch 3/170] [Batch 440/448] [D_gan loss: 0.159761, acc_gan:  93%] [D_cl loss: 0.068896, acc_cl:  10%] [G_gan loss: 3.581174, G_cl: 2.417776, recon: 0.175489] time: 5:42:49.687332 \n","[Epoch 3/170] [Batch 445/448] [D_gan loss: 0.110110, acc_gan:  95%] [D_cl loss: 0.056132, acc_cl:  12%] [G_gan loss: 3.972723, G_cl: 2.452311, recon: 0.178848] time: 5:43:45.310231 \n","[Epoch 4/170] [Batch 0/448] [D_gan loss: 0.116335, acc_gan:  95%] [D_cl loss: 0.057830, acc_cl:   7%] [G_gan loss: 4.757274, G_cl: 2.533768, recon: 0.179429] time: 5:44:27.152796 \n","*** TEST *** [D_gan accuracy : 0.765625 ] [D_cl accuracy : 0.40625 ]\n","[Epoch 4/170] [Batch 5/448] [D_gan loss: 0.077825, acc_gan:  97%] [D_cl loss: 0.054026, acc_cl:  10%] [G_gan loss: 4.326783, G_cl: 2.338996, recon: 0.171010] time: 5:45:24.577360 \n","[Epoch 4/170] [Batch 10/448] [D_gan loss: 0.074958, acc_gan:  98%] [D_cl loss: 0.060538, acc_cl:  19%] [G_gan loss: 5.221832, G_cl: 2.200144, recon: 0.177216] time: 5:46:22.283608 \n","[Epoch 4/170] [Batch 15/448] [D_gan loss: 0.009726, acc_gan:  99%] [D_cl loss: 0.048235, acc_cl:  14%] [G_gan loss: 5.810106, G_cl: 2.265574, recon: 0.191890] time: 5:47:17.697200 \n","[Epoch 4/170] [Batch 20/448] [D_gan loss: 0.072733, acc_gan:  96%] [D_cl loss: 0.053467, acc_cl:  19%] [G_gan loss: 5.576775, G_cl: 2.336781, recon: 0.193728] time: 5:48:15.448189 \n","[Epoch 4/170] [Batch 25/448] [D_gan loss: 0.205343, acc_gan:  93%] [D_cl loss: 0.062506, acc_cl:  11%] [G_gan loss: 5.035538, G_cl: 2.354948, recon: 0.214172] time: 5:49:12.283902 \n","[Epoch 4/170] [Batch 30/448] [D_gan loss: 0.065799, acc_gan:  96%] [D_cl loss: 0.053388, acc_cl:  14%] [G_gan loss: 5.047144, G_cl: 2.264110, recon: 0.217898] time: 5:50:06.426344 \n","[Epoch 4/170] [Batch 35/448] [D_gan loss: 0.106597, acc_gan:  95%] [D_cl loss: 0.056827, acc_cl:   6%] [G_gan loss: 4.834160, G_cl: 2.315593, recon: 0.248531] time: 5:51:01.976039 \n","[Epoch 4/170] [Batch 40/448] [D_gan loss: 0.101792, acc_gan:  95%] [D_cl loss: 0.058394, acc_cl:  15%] [G_gan loss: 4.831068, G_cl: 2.331374, recon: 0.228292] time: 5:51:58.768031 \n","[Epoch 4/170] [Batch 45/448] [D_gan loss: 0.071519, acc_gan:  97%] [D_cl loss: 0.054430, acc_cl:  14%] [G_gan loss: 4.457903, G_cl: 2.436908, recon: 0.228999] time: 5:52:55.619678 \n","[Epoch 4/170] [Batch 50/448] [D_gan loss: 0.040664, acc_gan:  98%] [D_cl loss: 0.052301, acc_cl:   8%] [G_gan loss: 5.252462, G_cl: 2.301364, recon: 0.229117] time: 5:53:51.267766 \n","[Epoch 4/170] [Batch 55/448] [D_gan loss: 0.064853, acc_gan:  98%] [D_cl loss: 0.058294, acc_cl:   8%] [G_gan loss: 4.990488, G_cl: 2.282233, recon: 0.222214] time: 5:54:47.852028 \n","[Epoch 4/170] [Batch 60/448] [D_gan loss: 0.123379, acc_gan:  95%] [D_cl loss: 0.057775, acc_cl:  15%] [G_gan loss: 3.663455, G_cl: 2.569142, recon: 0.213534] time: 5:55:44.778375 \n","[Epoch 4/170] [Batch 65/448] [D_gan loss: 0.058709, acc_gan:  97%] [D_cl loss: 0.049306, acc_cl:  10%] [G_gan loss: 5.030167, G_cl: 2.384942, recon: 0.197715] time: 5:56:41.508210 \n","[Epoch 4/170] [Batch 70/448] [D_gan loss: 0.063177, acc_gan:  98%] [D_cl loss: 0.052213, acc_cl:  15%] [G_gan loss: 4.877369, G_cl: 2.433292, recon: 0.194050] time: 5:57:38.141155 \n","[Epoch 4/170] [Batch 75/448] [D_gan loss: 0.036436, acc_gan:  98%] [D_cl loss: 0.049533, acc_cl:   6%] [G_gan loss: 5.559160, G_cl: 2.326482, recon: 0.185847] time: 5:58:35.710438 \n","[Epoch 4/170] [Batch 80/448] [D_gan loss: 0.013840, acc_gan:  99%] [D_cl loss: 0.045472, acc_cl:  10%] [G_gan loss: 5.987345, G_cl: 2.201374, recon: 0.180382] time: 5:59:32.402045 \n","[Epoch 4/170] [Batch 85/448] [D_gan loss: 0.298961, acc_gan:  88%] [D_cl loss: 0.086278, acc_cl:   4%] [G_gan loss: 2.328044, G_cl: 2.503364, recon: 0.173961] time: 6:00:28.950235 \n","[Epoch 4/170] [Batch 90/448] [D_gan loss: 0.152032, acc_gan:  95%] [D_cl loss: 0.060890, acc_cl:   8%] [G_gan loss: 4.311913, G_cl: 2.461702, recon: 0.185293] time: 6:01:26.833071 \n","[Epoch 4/170] [Batch 95/448] [D_gan loss: 0.132298, acc_gan:  94%] [D_cl loss: 0.055110, acc_cl:   8%] [G_gan loss: 3.430420, G_cl: 2.473147, recon: 0.169088] time: 6:02:24.878316 \n","[Epoch 4/170] [Batch 100/448] [D_gan loss: 0.108534, acc_gan:  95%] [D_cl loss: 0.057972, acc_cl:  12%] [G_gan loss: 3.645151, G_cl: 2.258423, recon: 0.183117] time: 6:03:21.320037 \n","[Epoch 4/170] [Batch 105/448] [D_gan loss: 0.063694, acc_gan:  98%] [D_cl loss: 0.057300, acc_cl:  13%] [G_gan loss: 4.395916, G_cl: 2.418190, recon: 0.194575] time: 6:04:16.691926 \n","[Epoch 4/170] [Batch 110/448] [D_gan loss: 0.228771, acc_gan:  92%] [D_cl loss: 0.077103, acc_cl:   8%] [G_gan loss: 3.191949, G_cl: 2.498437, recon: 0.195801] time: 6:05:12.169341 \n","[Epoch 4/170] [Batch 115/448] [D_gan loss: 0.182174, acc_gan:  94%] [D_cl loss: 0.070237, acc_cl:  14%] [G_gan loss: 3.301020, G_cl: 2.438495, recon: 0.185190] time: 6:06:09.982419 \n"]}]},{"cell_type":"markdown","source":["**until here everything seems be ok**"],"metadata":{"id":"8_Ua5uEH1f3r"}},{"cell_type":"code","source":[""],"metadata":{"id":"T6KQAV0sw5b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gan = CCycleGAN()\n","#gan.train(epochs=200, batch_size=64, sample_interval=200)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"5blpknj1cU0L","executionInfo":{"status":"error","timestamp":1642548002083,"user_tz":300,"elapsed":22837,"user":{"displayName":"Franklin Samuel Sierra Jerez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64","userId":"07062157042636671418"}},"outputId":"e1a2e2fd-5562-40d8-e188-332438e0ba1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[">> loading fer2013 ...\n","> loaded train: 28709    - test: 7178\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-7edb90d4c58a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCCycleGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#gan.train(epochs=200, batch_size=64, sample_interval=200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-34-c494ce854327>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_rows, img_cols, channels, num_classes, latent_dim, PREFIX)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Configure data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fer2013'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-123b6013f983>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_name, img_res, path_csv, use_test_in_batch, normalize)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m## load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_internally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-123b6013f983>\u001b[0m in \u001b[0;36m_load_internally\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mleo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./images/leo_gray__crop_48_48.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m#batch de uno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleo_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int32'\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# 6 for neutral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"h6PpQ7nAfSKz"},"execution_count":null,"outputs":[]}]}